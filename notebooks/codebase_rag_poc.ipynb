{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import jedi\n",
    "import networkx as nx\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".venv.Lib.site-packages.skforecast.model_selection._utils.initialize_lags_grid\n",
      ".venv.Lib.site-packages.skforecast.utils.utils.initialize_lags\n",
      "Graph has 375 nodes and 1906 edges.\n",
      "Graph successfully pushed to Neo4j.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import jedi\n",
    "import networkx as nx\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "\n",
    "def find_python_files(root):\n",
    "    \"\"\"Yield all .py files under root, skipping any in a directory named 'tests'.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        # Remove any directories named 'tests' (case-insensitive) from traversal.\n",
    "        dirnames[:] = [d for d in dirnames if d.lower() != \"tests\"]\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.py'):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "def get_definitions(file_path):\n",
    "    \"\"\"\n",
    "    Use Jedi to extract definitions from a file.\n",
    "    Returns a dict mapping a unique id to a dict with properties:\n",
    "      id, name, type (class, function, or method), file, and line.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return {}\n",
    "    script = jedi.Script(source, path=file_path)\n",
    "    defs = script.get_names(all_scopes=True, definitions=True, references=False)\n",
    "    definitions = {}\n",
    "    for d in defs:\n",
    "        if d.type in ('function', 'class'):\n",
    "            full_name = d.full_name or f\"{os.path.basename(file_path)}:{d.name}@{d.line}\"\n",
    "            # For functions, if full_name contains a dot, assume it's a method.\n",
    "            if d.type == 'function' and '.' in (d.full_name or ''):\n",
    "                if 'initialize_lags' in d.full_name:\n",
    "                    print(d.full_name)\n",
    "                node_type = 'method'\n",
    "            else:\n",
    "                node_type = d.type  # 'function' or 'class'\n",
    "            definitions[full_name] = {\n",
    "                'id': full_name,\n",
    "                'name': d.name,\n",
    "                'type': node_type,\n",
    "                'file': file_path,\n",
    "                'line': d.line,\n",
    "            }\n",
    "    return definitions\n",
    "\n",
    "def add_parent_pointers(node, parent=None):\n",
    "    \"\"\"Recursively add a 'parent' attribute to AST nodes.\"\"\"\n",
    "    for child in ast.iter_child_nodes(node):\n",
    "        child.parent = node\n",
    "        add_parent_pointers(child, node)\n",
    "\n",
    "def get_call_nodes(file_path):\n",
    "    \"\"\"\n",
    "    Parse the AST of a file and return all Call nodes (plus the AST tree).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return [], None\n",
    "    tree = ast.parse(source, filename=file_path)\n",
    "    add_parent_pointers(tree)\n",
    "    calls = []\n",
    "    class CallVisitor(ast.NodeVisitor):\n",
    "        def visit_Call(self, node):\n",
    "            calls.append(node)\n",
    "            self.generic_visit(node)\n",
    "    CallVisitor().visit(tree)\n",
    "    return calls, tree\n",
    "\n",
    "def get_enclosing_definition(file_path, lineno, col_offset):\n",
    "    \"\"\"\n",
    "    Use Jedi to get the innermost definition that encloses the given position.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "        script = jedi.Script(source, path=file_path)\n",
    "        context = script.get_context(line=lineno, column=col_offset)\n",
    "        if context and context.type in ('function', 'class'):\n",
    "            return context\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def extract_call_target_name(call_node):\n",
    "    \"\"\"\n",
    "    Extract the target function name from a Call node.\n",
    "    If the function is called directly (e.g. foo()), return 'foo'.\n",
    "    If it's an attribute call (e.g. obj.bar()), return 'bar'.\n",
    "    \"\"\"\n",
    "    if isinstance(call_node.func, ast.Name):\n",
    "        return call_node.func.id\n",
    "    elif isinstance(call_node.func, ast.Attribute):\n",
    "        return call_node.func.attr\n",
    "    return None\n",
    "\n",
    "# ---------- Build the Call Graph ----------\n",
    "\n",
    "def build_call_graph(project_root):\n",
    "    \"\"\"\n",
    "    Build a call graph for all Python files under project_root.\n",
    "    Nodes represent definitions (functions, methods, classes) with properties.\n",
    "    Edges:\n",
    "      - \"call\": a call from one definition to a target (matched by name)\n",
    "      - \"nested\": when a definition is declared inside another.\n",
    "    Returns a NetworkX DiGraph.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    all_defs = {}\n",
    "\n",
    "    # Pass 1: Gather all definitions.\n",
    "    for file_path in find_python_files(project_root):\n",
    "        defs = get_definitions(file_path)\n",
    "        for def_id, info in defs.items():\n",
    "            all_defs[def_id] = info\n",
    "            G.add_node(def_id, **info)\n",
    "\n",
    "    # Pass 2: Add nested definition edges.\n",
    "    for file_path in find_python_files(project_root):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                source = f.read()\n",
    "            tree = ast.parse(source, filename=file_path)\n",
    "            add_parent_pointers(tree)\n",
    "        except Exception:\n",
    "            continue\n",
    "        # Use AST walk to find nested definitions.\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "                try:\n",
    "                    script = jedi.Script(source, path=file_path)\n",
    "                    defs = script.get_names(all_scopes=True, definitions=True, references=False)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                current_id = None\n",
    "                for d in defs:\n",
    "                    if d.name == node.name and d.line == node.lineno:\n",
    "                        current_id = d.full_name or f\"{os.path.basename(file_path)}:{d.name}@{d.line}\"\n",
    "                        break\n",
    "                parent = getattr(node, 'parent', None)\n",
    "                parent_id = None\n",
    "                while parent:\n",
    "                    if isinstance(parent, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "                        for pd in defs:\n",
    "                            if pd.name == parent.name and pd.line == parent.lineno:\n",
    "                                parent_id = pd.full_name or f\"{os.path.basename(file_path)}:{pd.name}@{pd.line}\"\n",
    "                                break\n",
    "                        if parent_id:\n",
    "                            break\n",
    "                    parent = getattr(parent, 'parent', None)\n",
    "                if current_id and parent_id:\n",
    "                    if parent_id in G.nodes and current_id in G.nodes:\n",
    "                        G.add_edge(parent_id, current_id, relation='nested')\n",
    "\n",
    "    # Pass 3: Add call edges.\n",
    "    for file_path in find_python_files(project_root):\n",
    "        calls, _ = get_call_nodes(file_path)\n",
    "        for call in calls:\n",
    "            target_name = extract_call_target_name(call)\n",
    "            if not target_name:\n",
    "                continue\n",
    "            caller_context = get_enclosing_definition(file_path, call.lineno, call.col_offset)\n",
    "            if not caller_context:\n",
    "                continue\n",
    "            caller_id = caller_context.full_name or f\"{os.path.basename(file_path)}:{caller_context.name}@{caller_context.line}\"\n",
    "            candidate_ids = [did for did, info in all_defs.items() if info['name'] == target_name]\n",
    "            for candidate_id in candidate_ids:\n",
    "                if caller_id in G.nodes and candidate_id in G.nodes:\n",
    "                    G.add_edge(caller_id, candidate_id, relation='call')\n",
    "    return G\n",
    "\n",
    "# ---------- Neo4j Integration ----------\n",
    "\n",
    "def push_graph_to_neo4j(G, uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"mike_pass\"):\n",
    "    \"\"\"\n",
    "    Push the NetworkX graph G to a Neo4j database.\n",
    "    Each node is created with a label according to its type:\n",
    "      - Class, Function, or Method.\n",
    "    Each edge uses the relationship type from its 'relation' property.\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        # Clear existing nodes (use with caution in production)\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        # Create nodes.\n",
    "        for node_id, data in G.nodes(data=True):\n",
    "            if data.get('type') == 'class':\n",
    "                label = \"Class\"\n",
    "            elif data.get('type') == 'method':\n",
    "                label = \"Method\"\n",
    "            else:\n",
    "                label = \"Function\"\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                CREATE (n:{label} {{id: $id, name: $name, file: $file, line: $line}})\n",
    "                \"\"\",\n",
    "                id=node_id,\n",
    "                name=data.get('name'),\n",
    "                file=data.get('file'),\n",
    "                line=data.get('line')\n",
    "            )\n",
    "        # Create relationships.\n",
    "        for source, target, data in G.edges(data=True):\n",
    "            rel_type = data.get('relation', 'call').upper()  # e.g. 'CALL' or 'NESTED'\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                MATCH (a {{id: $source}}), (b {{id: $target}})\n",
    "                CREATE (a)-[r:{rel_type}]->(b)\n",
    "                \"\"\",\n",
    "                source=source,\n",
    "                target=target\n",
    "            )\n",
    "    driver.close()\n",
    "    print(\"Graph successfully pushed to Neo4j.\")\n",
    "\n",
    "# ---------- Notebook Main Execution ----------\n",
    "\n",
    "# Set the project root directory (make sure to exclude tests directories)\n",
    "project_root = \"C:/Projects/codebase_rag/.venv/Lib/site-packages/skforecast\"  # <-- Update this!\n",
    "graph = build_call_graph(project_root)\n",
    "print(f\"Graph has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "push_graph_to_neo4j(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 375 nodes and 1906 edges.\n",
      "Graph successfully pushed to Neo4j.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import jedi\n",
    "import networkx as nx\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "\n",
    "def find_python_files(root):\n",
    "    \"\"\"Yield all .py files under root, skipping any in a directory named 'tests'.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        # Remove any directories named 'tests' (case-insensitive) from traversal.\n",
    "        dirnames[:] = [d for d in dirnames if d.lower() != \"tests\"]\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.py'):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "def add_parent_pointers(node, parent=None):\n",
    "    \"\"\"Recursively add a 'parent' attribute to AST nodes.\"\"\"\n",
    "    for child in ast.iter_child_nodes(node):\n",
    "        child.parent = node\n",
    "        add_parent_pointers(child, node)\n",
    "\n",
    "def is_method_in_file(file_path, name, lineno):\n",
    "    \"\"\"\n",
    "    Parse file_path and return True if there is a function definition\n",
    "    with the given name and lineno that is nested inside a ClassDef.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "        tree = ast.parse(source, filename=file_path)\n",
    "        add_parent_pointers(tree)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in is_method_in_file for {file_path}: {e}\")\n",
    "        return False\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) and node.name == name and node.lineno == lineno:\n",
    "            parent = getattr(node, 'parent', None)\n",
    "            while parent:\n",
    "                if isinstance(parent, ast.ClassDef):\n",
    "                    return True\n",
    "                parent = getattr(parent, 'parent', None)\n",
    "    return False\n",
    "\n",
    "def get_definitions(file_path):\n",
    "    \"\"\"\n",
    "    Use Jedi to extract definitions from a file.\n",
    "    Returns a dict mapping a unique id to a dict with properties:\n",
    "      id, name, type (class, function, or method), file, and line.\n",
    "    \"\"\"\n",
    "    definitions = {}\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return definitions\n",
    "    script = jedi.Script(source, path=file_path)\n",
    "    defs = script.get_names(all_scopes=True, definitions=True, references=False)\n",
    "    for d in defs:\n",
    "        if d.type in ('function', 'class'):\n",
    "            full_name = d.full_name or f\"{os.path.basename(file_path)}:{d.name}@{d.line}\"\n",
    "            if d.type == 'function':\n",
    "                # Check using AST whether this function is nested in a class.\n",
    "                if is_method_in_file(file_path, d.name, d.line):\n",
    "                    node_type = 'method'\n",
    "                else:\n",
    "                    node_type = 'function'\n",
    "            else:\n",
    "                node_type = 'class'\n",
    "            definitions[full_name] = {\n",
    "                'id': full_name,\n",
    "                'name': d.name,\n",
    "                'type': node_type,\n",
    "                'file': file_path,\n",
    "                'line': d.line,\n",
    "            }\n",
    "    return definitions\n",
    "\n",
    "def get_call_nodes(file_path):\n",
    "    \"\"\"\n",
    "    Parse the AST of a file and return all Call nodes (plus the AST tree).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return [], None\n",
    "    tree = ast.parse(source, filename=file_path)\n",
    "    add_parent_pointers(tree)\n",
    "    calls = []\n",
    "    class CallVisitor(ast.NodeVisitor):\n",
    "        def visit_Call(self, node):\n",
    "            calls.append(node)\n",
    "            self.generic_visit(node)\n",
    "    CallVisitor().visit(tree)\n",
    "    return calls, tree\n",
    "\n",
    "def get_enclosing_definition(file_path, lineno, col_offset):\n",
    "    \"\"\"\n",
    "    Use Jedi to get the innermost definition that encloses the given position.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "        script = jedi.Script(source, path=file_path)\n",
    "        context = script.get_context(line=lineno, column=col_offset)\n",
    "        if context and context.type in ('function', 'class'):\n",
    "            return context\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def extract_call_target_name(call_node):\n",
    "    \"\"\"\n",
    "    Extract the target function name from a Call node.\n",
    "    If the function is called directly (e.g. foo()), return 'foo'.\n",
    "    If it's an attribute call (e.g. obj.bar()), return 'bar'.\n",
    "    \"\"\"\n",
    "    if isinstance(call_node.func, ast.Name):\n",
    "        return call_node.func.id\n",
    "    elif isinstance(call_node.func, ast.Attribute):\n",
    "        return call_node.func.attr\n",
    "    return None\n",
    "\n",
    "# ---------- Build the Call Graph ----------\n",
    "\n",
    "def build_call_graph(project_root):\n",
    "    \"\"\"\n",
    "    Build a call graph for all Python files under project_root.\n",
    "    Nodes represent definitions (functions, methods, classes) with properties.\n",
    "    Edges:\n",
    "      - \"call\": a call from one definition to a target (matched by name)\n",
    "      - \"nested\": when a definition is declared inside another.\n",
    "    Returns a NetworkX DiGraph.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    all_defs = {}\n",
    "\n",
    "    # Pass 1: Gather all definitions.\n",
    "    for file_path in find_python_files(project_root):\n",
    "        defs = get_definitions(file_path)\n",
    "        for def_id, info in defs.items():\n",
    "            all_defs[def_id] = info\n",
    "            G.add_node(def_id, **info)\n",
    "\n",
    "    # Pass 2: Add nested definition edges.\n",
    "    for file_path in find_python_files(project_root):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                source = f.read()\n",
    "            tree = ast.parse(source, filename=file_path)\n",
    "            add_parent_pointers(tree)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "                try:\n",
    "                    script = jedi.Script(source, path=file_path)\n",
    "                    defs = script.get_names(all_scopes=True, definitions=True, references=False)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                current_id = None\n",
    "                for d in defs:\n",
    "                    if d.name == node.name and d.line == node.lineno:\n",
    "                        current_id = d.full_name or f\"{os.path.basename(file_path)}:{d.name}@{d.line}\"\n",
    "                        break\n",
    "                parent = getattr(node, 'parent', None)\n",
    "                parent_id = None\n",
    "                while parent:\n",
    "                    if isinstance(parent, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "                        for pd in defs:\n",
    "                            if pd.name == parent.name and pd.line == parent.lineno:\n",
    "                                parent_id = pd.full_name or f\"{os.path.basename(file_path)}:{pd.name}@{pd.line}\"\n",
    "                                break\n",
    "                        if parent_id:\n",
    "                            break\n",
    "                    parent = getattr(parent, 'parent', None)\n",
    "                if current_id and parent_id:\n",
    "                    if parent_id in G.nodes and current_id in G.nodes:\n",
    "                        G.add_edge(parent_id, current_id, relation='nested')\n",
    "\n",
    "    # Pass 3: Add call edges.\n",
    "    for file_path in find_python_files(project_root):\n",
    "        calls, _ = get_call_nodes(file_path)\n",
    "        for call in calls:\n",
    "            target_name = extract_call_target_name(call)\n",
    "            if not target_name:\n",
    "                continue\n",
    "            caller_context = get_enclosing_definition(file_path, call.lineno, call.col_offset)\n",
    "            if not caller_context:\n",
    "                continue\n",
    "            caller_id = caller_context.full_name or f\"{os.path.basename(file_path)}:{caller_context.name}@{caller_context.line}\"\n",
    "            candidate_ids = [did for did, info in all_defs.items() if info['name'] == target_name]\n",
    "            for candidate_id in candidate_ids:\n",
    "                if caller_id in G.nodes and candidate_id in G.nodes:\n",
    "                    G.add_edge(caller_id, candidate_id, relation='call')\n",
    "    return G\n",
    "\n",
    "# ---------- Neo4j Integration ----------\n",
    "\n",
    "def push_graph_to_neo4j(G, uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"Mike_pass\"):\n",
    "    \"\"\"\n",
    "    Push the NetworkX graph G to a Neo4j database.\n",
    "    Each node is created with a label according to its type:\n",
    "      - Class, Function, or Method.\n",
    "    Each edge uses the relationship type from its 'relation' property.\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        # Clear existing nodes (use with caution in production)\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        # Create nodes.\n",
    "        for node_id, data in G.nodes(data=True):\n",
    "            if data.get('type') == 'class':\n",
    "                label = \"Class\"\n",
    "            elif data.get('type') == 'method':\n",
    "                label = \"Method\"\n",
    "            else:\n",
    "                label = \"Function\"\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                CREATE (n:{label} {{id: $id, name: $name, file: $file, line: $line}})\n",
    "                \"\"\",\n",
    "                id=node_id,\n",
    "                name=data.get('name'),\n",
    "                file=data.get('file'),\n",
    "                line=data.get('line')\n",
    "            )\n",
    "        # Create relationships.\n",
    "        for source, target, data in G.edges(data=True):\n",
    "            rel_type = data.get('relation', 'call').upper()  # e.g. 'CALL' or 'NESTED'\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                MATCH (a {{id: $source}}), (b {{id: $target}})\n",
    "                CREATE (a)-[r:{rel_type}]->(b)\n",
    "                \"\"\",\n",
    "                source=source,\n",
    "                target=target\n",
    "            )\n",
    "    driver.close()\n",
    "    print(\"Graph successfully pushed to Neo4j.\")\n",
    "\n",
    "# ---------- Notebook Main Execution ----------\n",
    "\n",
    "# Set the project root directory (make sure to exclude tests directories)\n",
    "project_root = \"C:/Projects/codebase_rag/.venv/Lib/site-packages/skforecast\"  # <-- Update this!\n",
    "graph = build_call_graph(project_root)\n",
    "print(f\"Graph has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "push_graph_to_neo4j(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 375 nodes and 1906 edges.\n",
      "Graph successfully pushed to Neo4j.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import jedi\n",
    "import networkx as nx\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "\n",
    "def find_python_files(root):\n",
    "    \"\"\"Yield all .py files under root, skipping any in a directory named 'tests'.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        # Remove any directories named 'tests' (case-insensitive) from traversal.\n",
    "        dirnames[:] = [d for d in dirnames if d.lower() != \"tests\"]\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.py'):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "def add_parent_pointers(node, parent=None):\n",
    "    \"\"\"Recursively add a 'parent' attribute to AST nodes.\"\"\"\n",
    "    for child in ast.iter_child_nodes(node):\n",
    "        child.parent = node\n",
    "        add_parent_pointers(child, node)\n",
    "\n",
    "def is_method_in_file(file_path, name, lineno):\n",
    "    \"\"\"\n",
    "    Parse file_path and return True if there is a function definition\n",
    "    with the given name and lineno that is nested inside a ClassDef.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "        tree = ast.parse(source, filename=file_path)\n",
    "        add_parent_pointers(tree)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in is_method_in_file for {file_path}: {e}\")\n",
    "        return False\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) and node.name == name and node.lineno == lineno:\n",
    "            parent = getattr(node, 'parent', None)\n",
    "            while parent:\n",
    "                if isinstance(parent, ast.ClassDef):\n",
    "                    return True\n",
    "                parent = getattr(parent, 'parent', None)\n",
    "    return False\n",
    "\n",
    "def get_definitions(file_path):\n",
    "    \"\"\"\n",
    "    Use Jedi to extract definitions from a file.\n",
    "    Returns a dict mapping a unique id to a dict with properties:\n",
    "      id, name, type (class, function, or method), file, line, and code.\n",
    "    \"\"\"\n",
    "    definitions = {}\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return definitions\n",
    "    try:\n",
    "        tree = ast.parse(source, filename=file_path)\n",
    "    except Exception as e:\n",
    "        tree = None\n",
    "    script = jedi.Script(source, path=file_path)\n",
    "    defs = script.get_names(all_scopes=True, definitions=True, references=False)\n",
    "    for d in defs:\n",
    "        if d.type in ('function', 'class'):\n",
    "            full_name = d.full_name or f\"{os.path.basename(file_path)}:{d.name}@{d.line}\"\n",
    "            if d.type == 'function':\n",
    "                # Check using AST whether this function is nested in a class.\n",
    "                if is_method_in_file(file_path, d.name, d.line):\n",
    "                    node_type = 'method'\n",
    "                else:\n",
    "                    node_type = 'function'\n",
    "            else:\n",
    "                node_type = 'class'\n",
    "            code_segment = \"\"\n",
    "            if tree is not None:\n",
    "                for node in ast.walk(tree):\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)) and node.name == d.name and node.lineno == d.line:\n",
    "                        code_segment = ast.get_source_segment(source, node)\n",
    "                        break\n",
    "            definitions[full_name] = {\n",
    "                'id': full_name,\n",
    "                'name': d.name,\n",
    "                'type': node_type,\n",
    "                'file': file_path,\n",
    "                'line': d.line,\n",
    "                'code': code_segment,\n",
    "            }\n",
    "    return definitions\n",
    "\n",
    "def get_call_nodes(file_path):\n",
    "    \"\"\"\n",
    "    Parse the AST of a file and return all Call nodes (plus the AST tree).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return [], None\n",
    "    tree = ast.parse(source, filename=file_path)\n",
    "    add_parent_pointers(tree)\n",
    "    calls = []\n",
    "    class CallVisitor(ast.NodeVisitor):\n",
    "        def visit_Call(self, node):\n",
    "            calls.append(node)\n",
    "            self.generic_visit(node)\n",
    "    CallVisitor().visit(tree)\n",
    "    return calls, tree\n",
    "\n",
    "def get_enclosing_definition(file_path, lineno, col_offset):\n",
    "    \"\"\"\n",
    "    Use Jedi to get the innermost definition that encloses the given position.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "        script = jedi.Script(source, path=file_path)\n",
    "        context = script.get_context(line=lineno, column=col_offset)\n",
    "        if context and context.type in ('function', 'class'):\n",
    "            return context\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def extract_call_target_name(call_node):\n",
    "    \"\"\"\n",
    "    Extract the target function name from a Call node.\n",
    "    If the function is called directly (e.g. foo()), return 'foo'.\n",
    "    If it's an attribute call (e.g. obj.bar()), return 'bar'.\n",
    "    \"\"\"\n",
    "    if isinstance(call_node.func, ast.Name):\n",
    "        return call_node.func.id\n",
    "    elif isinstance(call_node.func, ast.Attribute):\n",
    "        return call_node.func.attr\n",
    "    return None\n",
    "\n",
    "# ---------- Build the Call Graph ----------\n",
    "\n",
    "def build_call_graph(project_root):\n",
    "    \"\"\"\n",
    "    Build a call graph for all Python files under project_root.\n",
    "    Nodes represent definitions (functions, methods, classes) with properties.\n",
    "    Edges:\n",
    "      - \"call\": a call from one definition to a target (matched by name)\n",
    "      - \"nested\": when a definition is declared inside another.\n",
    "    Returns a NetworkX DiGraph.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    all_defs = {}\n",
    "\n",
    "    # Pass 1: Gather all definitions.\n",
    "    for file_path in find_python_files(project_root):\n",
    "        defs = get_definitions(file_path)\n",
    "        for def_id, info in defs.items():\n",
    "            all_defs[def_id] = info\n",
    "            G.add_node(def_id, **info)\n",
    "\n",
    "    # Pass 2: Add nested definition edges.\n",
    "    for file_path in find_python_files(project_root):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                source = f.read()\n",
    "            tree = ast.parse(source, filename=file_path)\n",
    "            add_parent_pointers(tree)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "                try:\n",
    "                    script = jedi.Script(source, path=file_path)\n",
    "                    defs = script.get_names(all_scopes=True, definitions=True, references=False)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                current_id = None\n",
    "                for d in defs:\n",
    "                    if d.name == node.name and d.line == node.lineno:\n",
    "                        current_id = d.full_name or f\"{os.path.basename(file_path)}:{d.name}@{d.line}\"\n",
    "                        break\n",
    "                parent = getattr(node, 'parent', None)\n",
    "                parent_id = None\n",
    "                while parent:\n",
    "                    if isinstance(parent, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "                        for pd in defs:\n",
    "                            if pd.name == parent.name and pd.line == parent.lineno:\n",
    "                                parent_id = pd.full_name or f\"{os.path.basename(file_path)}:{pd.name}@{pd.line}\"\n",
    "                                break\n",
    "                        if parent_id:\n",
    "                            break\n",
    "                    parent = getattr(parent, 'parent', None)\n",
    "                if current_id and parent_id:\n",
    "                    if parent_id in G.nodes and current_id in G.nodes:\n",
    "                        G.add_edge(parent_id, current_id, relation='nested')\n",
    "\n",
    "    # Pass 3: Add call edges.\n",
    "    for file_path in find_python_files(project_root):\n",
    "        calls, _ = get_call_nodes(file_path)\n",
    "        for call in calls:\n",
    "            target_name = extract_call_target_name(call)\n",
    "            if not target_name:\n",
    "                continue\n",
    "            caller_context = get_enclosing_definition(file_path, call.lineno, call.col_offset)\n",
    "            if not caller_context:\n",
    "                continue\n",
    "            caller_id = caller_context.full_name or f\"{os.path.basename(file_path)}:{caller_context.name}@{caller_context.line}\"\n",
    "            candidate_ids = [did for did, info in all_defs.items() if info['name'] == target_name]\n",
    "            for candidate_id in candidate_ids:\n",
    "                if caller_id in G.nodes and candidate_id in G.nodes:\n",
    "                    G.add_edge(caller_id, candidate_id, relation='call')\n",
    "    return G\n",
    "\n",
    "# ---------- Neo4j Integration ----------\n",
    "\n",
    "def push_graph_to_neo4j(G, uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"Mike_pass\"):\n",
    "    \"\"\"\n",
    "    Push the NetworkX graph G to a Neo4j database.\n",
    "    Each node is created with a label according to its type:\n",
    "      - Class, Function, or Method.\n",
    "    Each edge uses the relationship type from its 'relation' property.\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        # Clear existing nodes (use with caution in production)\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        # Create nodes.\n",
    "        for node_id, data in G.nodes(data=True):\n",
    "            if data.get('type') == 'class':\n",
    "                label = \"Class\"\n",
    "            elif data.get('type') == 'method':\n",
    "                label = \"Method\"\n",
    "            else:\n",
    "                label = \"Function\"\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                CREATE (n:{label} {{id: $id, name: $name, file: $file, line: $line, code: $code}})\n",
    "                \"\"\",\n",
    "                id=node_id,\n",
    "                name=data.get('name'),\n",
    "                file=data.get('file'),\n",
    "                line=data.get('line'),\n",
    "                code=data.get('code')\n",
    "            )\n",
    "        # Create relationships.\n",
    "        for source, target, data in G.edges(data=True):\n",
    "            rel_type = data.get('relation', 'call').upper()  # e.g. 'CALL' or 'NESTED'\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                MATCH (a {{id: $source}}), (b {{id: $target}})\n",
    "                CREATE (a)-[r:{rel_type}]->(b)\n",
    "                \"\"\",\n",
    "                source=source,\n",
    "                target=target\n",
    "            )\n",
    "    driver.close()\n",
    "    print(\"Graph successfully pushed to Neo4j.\")\n",
    "\n",
    "# ---------- Notebook Main Execution ----------\n",
    "\n",
    "# Set the project root directory (make sure to exclude tests directories)\n",
    "project_root = \"C:/Projects/codebase_rag/.venv/Lib/site-packages/skforecast\"  # <-- Update this!\n",
    "graph = build_call_graph(project_root)\n",
    "print(f\"Graph has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "push_graph_to_neo4j(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph, GraphCypherQAChain\n",
    "\n",
    "kg = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\", username=\"neo4j\", password=\"Mike_pass\", database=\"neo4j\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties: Class {file: STRING, id: STRING, line:\n",
      "INTEGER, name: STRING, code: STRING} Function {file: STRING,\n",
      "id: STRING, line: INTEGER, name: STRING, code: STRING}\n",
      "Method {file: STRING, id: STRING, line: INTEGER, name:\n",
      "STRING, code: STRING} Relationship properties:  The\n",
      "relationships: (:Class)-[:NESTED]->(:Method)\n",
      "(:Class)-[:CALL]->(:Function) (:Function)-[:CALL]->(:Method)\n",
      "(:Function)-[:CALL]->(:Function)\n",
      "(:Function)-[:CALL]->(:Class)\n",
      "(:Function)-[:NESTED]->(:Function)\n",
      "(:Method)-[:CALL]->(:Method) (:Method)-[:CALL]->(:Class)\n",
      "(:Method)-[:CALL]->(:Function)\n",
      "(:Method)-[:NESTED]->(:Method)\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "print(textwrap.fill(kg.schema, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Always return the question too.\n",
    "Schema:\n",
    "{schema}\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "Examples: Here are a few examples of generated Cypher statements for particular questions:\n",
    "\n",
    "# What are the nodes directly related to example_node?\n",
    "MATCH (node)-[]-(other_nodes)\n",
    "WHERE node.name = 'example.node'\n",
    "RETURN other_nodes\n",
    "\n",
    "# What are the function or method nodes that have a relationship towards example_node?\n",
    "MATCH (node)<-[]-(other_nodes:Function|Method)\n",
    "WHERE node.name = 'example.node'\n",
    "RETURN other_nodes\n",
    "\n",
    "# What are the function or method nodes that call example_node?\n",
    "MATCH (node)<-[:CALL]-(other_nodes:Function|Method)\n",
    "WHERE node.name = 'example.node'\n",
    "RETURN other_nodes\n",
    "\n",
    "# What is the file that stores example?\n",
    "MATCH (node)\n",
    "WHERE node.name = 'example'\n",
    "RETURN node.file\n",
    "\n",
    "# Inside what function or method is example_function defined?\n",
    "MATCH (node)<-[r:NESTED_IN]-(other_node)\n",
    "WHERE node.name = 'example_function'\n",
    "RETURN other_node\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "QA_GENERATION_TEMPLATE = \"\"\"\n",
    "You are an assistant specialized in retrieving and interpreting code snippets from a graph database.\n",
    "Based on the user's question and the provided context, identify relevant pieces of code within the node properties and present them clearly.\n",
    "\n",
    "User's Question:\n",
    "{question}\n",
    "\n",
    "Context from Database:\n",
    "{context}\n",
    "\n",
    "Extracted Code Snippets:\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], \n",
    "    template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "QA_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=QA_GENERATION_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"codebase_rag\"\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_TRACING: true\n",
      "LANGSMITH_ENDPOINT: https://api.smith.langchain.com\n",
      "LANGSMITH_API_KEY: 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\n",
      "LANGSMITH_PROJECT: codebase_rag\n",
      "OPENAI_API_KEY: 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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"LANGSMITH_TRACING:\", os.getenv(\"LANGSMITH_TRACING\"))\n",
    "print(\"LANGSMITH_ENDPOINT:\", os.getenv(\"LANGSMITH_ENDPOINT\"))\n",
    "print(\"LANGSMITH_API_KEY:\", os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "print(\"LANGSMITH_PROJECT:\", os.getenv(\"LANGSMITH_PROJECT\"))\n",
    "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypherChain = GraphCypherQAChain.from_llm(\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    "    graph=kg,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT, \n",
    "    # qa_prompt=QA_GENERATION_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"I want to modify the ForecasterRecursive class to receive an argument that allows to specify \n",
    "a string argument to select different 'differentiator's for the differentiation it implements.\n",
    "Can you suggest what changes need to be made? Include code snippets\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (class:Class)\n",
      "WHERE class.name = 'ForecasterRecursive'\n",
      "RETURN class.code\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'class.code': 'class ForecasterRecursive(ForecasterBase):\\n    \"\"\"\\n    This class turns any regressor compatible with the scikit-learn API into a\\n    recursive autoregressive (multi-step) forecaster.\\n    \\n    Parameters\\n    ----------\\n    regressor : regressor or pipeline compatible with the scikit-learn API\\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\\n    lags : int, list, numpy ndarray, range, default `None`\\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\\n    \\n        - `int`: include lags from 1 to `lags` (included).\\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \\n        `lags`, all elements must be int.\\n        - `None`: no lags are included as predictors. \\n    window_features : object, list, default `None`\\n        Instance or list of instances used to create window features. Window features\\n        are created from the original time series and are included as predictors.\\n    transformer_y : object transformer (preprocessor), default `None`\\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\\n        The transformation is applied to `y` before training the forecaster. \\n    transformer_exog : object transformer (preprocessor), default `None`\\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\\n        preprocessing API. The transformation is applied to `exog` before training the\\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\\n    weight_func : Callable, default `None`\\n        Function that defines the individual weights for each sample based on the\\n        index. For example, a function that assigns a lower weight to certain dates.\\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\\n        method. The resulting `sample_weight` cannot have negative values.\\n    differentiation : int, default `None`\\n        Order of differencing applied to the time series before training the forecaster.\\n        If `None`, no differencing is applied. The order of differentiation is the number\\n        of times the differencing operation is applied to a time series. Differencing\\n        involves computing the differences between consecutive data points in the series.\\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\\n        **WARNING: This argument is newly introduced and requires special attention. It\\n        is still experimental and may undergo changes.**\\n    fit_kwargs : dict, default `None`\\n        Additional arguments to be passed to the `fit` method of the regressor.\\n    binner_kwargs : dict, default `None`\\n        Additional arguments to pass to the `QuantileBinner` used to discretize \\n        the residuals into k bins according to the predicted values associated \\n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\\n        `random_state` and `dtype`. Argument `method` is passed internally to the\\n        fucntion `numpy.percentile`.\\n        **New in version 0.14.0**\\n    forecaster_id : str, int, default `None`\\n        Name used as an identifier of the forecaster.\\n    \\n    Attributes\\n    ----------\\n    regressor : regressor or pipeline compatible with the scikit-learn API\\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\\n    lags : numpy ndarray\\n        Lags used as predictors.\\n    lags_names : list\\n        Names of the lags used as predictors.\\n    max_lag : int\\n        Maximum lag included in `lags`.\\n    window_features : list\\n        Class or list of classes used to create window features.\\n    window_features_names : list\\n        Names of the window features to be included in the `X_train` matrix.\\n    window_features_class_names : list\\n        Names of the classes used to create the window features.\\n    max_size_window_features : int\\n        Maximum window size required by the window features.\\n    window_size : int\\n        The window size needed to create the predictors. It is calculated as the \\n        maximum value between `max_lag` and `max_size_window_features`. If \\n        differentiation is used, `window_size` is increased by n units equal to \\n        the order of differentiation so that predictors can be generated correctly.\\n    transformer_y : object transformer (preprocessor)\\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\\n        The transformation is applied to `y` before training the forecaster.\\n    transformer_exog : object transformer (preprocessor)\\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\\n        preprocessing API. The transformation is applied to `exog` before training the\\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\\n    weight_func : Callable\\n        Function that defines the individual weights for each sample based on the\\n        index. For example, a function that assigns a lower weight to certain dates.\\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\\n        method. The resulting `sample_weight` cannot have negative values.\\n    differentiation : int\\n        Order of differencing applied to the time series before training the forecaster.\\n        If `None`, no differencing is applied. The order of differentiation is the number\\n        of times the differencing operation is applied to a time series. Differencing\\n        involves computing the differences between consecutive data points in the series.\\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\\n        **WARNING: This argument is newly introduced and requires special attention. It\\n        is still experimental and may undergo changes.**\\n        **New in version 0.10.0**\\n    binner : sklearn.preprocessing.KBinsDiscretizer\\n        `KBinsDiscretizer` used to discretize residuals into k bins according \\n        to the predicted values associated with each residual.\\n        **New in version 0.12.0**\\n    binner_intervals_ : dict\\n        Intervals used to discretize residuals into k bins according to the predicted\\n        values associated with each residual.\\n        **New in version 0.12.0**\\n    binner_kwargs : dict\\n        Additional arguments to pass to the `QuantileBinner` used to discretize \\n        the residuals into k bins according to the predicted values associated \\n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\\n        `random_state` and `dtype`. Argument `method` is passed internally to the\\n        fucntion `numpy.percentile`.\\n        **New in version 0.14.0**\\n    source_code_weight_func : str\\n        Source code of the custom function used to create weights.\\n    differentiation : int\\n        Order of differencing applied to the time series before training the \\n        forecaster.\\n    differentiator : TimeSeriesDifferentiator\\n        Skforecast object used to differentiate the time series.\\n    last_window_ : pandas DataFrame\\n        This window represents the most recent data observed by the predictor\\n        during its training phase. It contains the values needed to predict the\\n        next step immediately after the training data. These values are stored\\n        in the original scale of the time series before undergoing any transformations\\n        or differentiation. When `differentiation` parameter is specified, the\\n        dimensions of the `last_window_` are expanded as many values as the order\\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\\n        `last_window_` will have 8 values.\\n    index_type_ : type\\n        Type of index of the input used in training.\\n    index_freq_ : str\\n        Frequency of Index of the input used in training.\\n    training_range_ : pandas Index\\n        First and last values of index of the data used during training.\\n    exog_in_ : bool\\n        If the forecaster has been trained using exogenous variable/s.\\n    exog_names_in_ : list\\n        Names of the exogenous variables used during training.\\n    exog_type_in_ : type\\n        Type of exogenous data (pandas Series or DataFrame) used in training.\\n    exog_dtypes_in_ : dict\\n        Type of each exogenous variable/s used in training. If `transformer_exog` \\n        is used, the dtypes are calculated before the transformation.\\n    X_train_window_features_names_out_ : list\\n        Names of the window features included in the matrix `X_train` created\\n        internally for training.\\n    X_train_exog_names_out_ : list\\n        Names of the exogenous variables included in the matrix `X_train` created\\n        internally for training. It can be different from `exog_names_in_` if\\n        some exogenous variables are transformed during the training process.\\n    X_train_features_names_out_ : list\\n        Names of columns of the matrix created internally for training.\\n    fit_kwargs : dict\\n        Additional arguments to be passed to the `fit` method of the regressor.\\n    in_sample_residuals_ : numpy ndarray\\n        Residuals of the model when predicting training data. Only stored up to\\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\\n        the transformed scale. If `differentiation` is not `None`, residuals are\\n        stored after differentiation.\\n    in_sample_residuals_by_bin_ : dict\\n        In sample residuals binned according to the predicted value each residual\\n        is associated with. If `transformer_y` is not `None`, residuals are stored\\n        in the transformed scale. If `differentiation` is not `None`, residuals are\\n        stored after differentiation. The number of residuals stored per bin is\\n        limited to `10_000 // self.binner.n_bins_`.\\n        **New in version 0.14.0**\\n    out_sample_residuals_ : numpy ndarray\\n        Residuals of the model when predicting non training data. Only stored up to\\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\\n        the transformed scale. If `differentiation` is not `None`, residuals are\\n        stored after differentiation.\\n    out_sample_residuals_by_bin_ : dict\\n        Out of sample residuals binned according to the predicted value each residual\\n        is associated with. If `transformer_y` is not `None`, residuals are stored\\n        in the transformed scale. If `differentiation` is not `None`, residuals are\\n        stored after differentiation. The number of residuals stored per bin is\\n        limited to `10_000 // self.binner.n_bins_`.\\n        **New in version 0.12.0**\\n    creation_date : str\\n        Date of creation.\\n    is_fitted : bool\\n        Tag to identify if the regressor has been fitted (trained).\\n    fit_date : str\\n        Date of last fit.\\n    skforecast_version : str\\n        Version of skforecast library used to create the forecaster.\\n    python_version : str\\n        Version of python used to create the forecaster.\\n    forecaster_id : str, int\\n        Name used as an identifier of the forecaster.\\n    \\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        regressor: object,\\n        lags: Optional[Union[int, list, np.ndarray, range]] = None,\\n        window_features: Optional[Union[object, list]] = None,\\n        transformer_y: Optional[object] = None,\\n        transformer_exog: Optional[object] = None,\\n        weight_func: Optional[Callable] = None,\\n        differentiation: Optional[int] = None,\\n        fit_kwargs: Optional[dict] = None,\\n        binner_kwargs: Optional[dict] = None,\\n        forecaster_id: Optional[Union[str, int]] = None\\n    ) -> None:\\n        \\n        self.regressor                          = copy(regressor)\\n        self.transformer_y                      = transformer_y\\n        self.transformer_exog                   = transformer_exog\\n        self.weight_func                        = weight_func\\n        self.source_code_weight_func            = None\\n        self.differentiation                    = differentiation\\n        self.differentiator                     = None\\n        self.last_window_                       = None\\n        self.index_type_                        = None\\n        self.index_freq_                        = None\\n        self.training_range_                    = None\\n        self.exog_in_                           = False\\n        self.exog_names_in_                     = None\\n        self.exog_type_in_                      = None\\n        self.exog_dtypes_in_                    = None\\n        self.X_train_window_features_names_out_ = None\\n        self.X_train_exog_names_out_            = None\\n        self.X_train_features_names_out_        = None\\n        self.in_sample_residuals_               = None\\n        self.out_sample_residuals_              = None\\n        self.in_sample_residuals_by_bin_        = None\\n        self.out_sample_residuals_by_bin_       = None\\n        self.creation_date                      = pd.Timestamp.today().strftime(\\'%Y-%m-%d %H:%M:%S\\')\\n        self.is_fitted                          = False\\n        self.fit_date                           = None\\n        self.skforecast_version                 = skforecast.__version__\\n        self.python_version                     = sys.version.split(\" \")[0]\\n        self.forecaster_id                      = forecaster_id\\n\\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\\n        self.window_features, self.window_features_names, self.max_size_window_features = (\\n            initialize_window_features(window_features)\\n        )\\n        if self.window_features is None and self.lags is None:\\n            raise ValueError(\\n                \"At least one of the arguments `lags` or `window_features` \"\\n                \"must be different from None. This is required to create the \"\\n                \"predictors used in training the forecaster.\"\\n            )\\n        \\n        self.window_size = max(\\n            [ws for ws in [self.max_lag, self.max_size_window_features] \\n             if ws is not None]\\n        )\\n        self.window_features_class_names = None\\n        if window_features is not None:\\n            self.window_features_class_names = [\\n                type(wf).__name__ for wf in self.window_features\\n            ] \\n\\n        self.binner_kwargs = binner_kwargs\\n        if binner_kwargs is None:\\n            self.binner_kwargs = {\\n                \\'n_bins\\': 10, \\'method\\': \\'linear\\', \\'subsample\\': 200000,\\n                \\'random_state\\': 789654, \\'dtype\\': np.float64\\n            }\\n        self.binner = QuantileBinner(**self.binner_kwargs)\\n        self.binner_intervals_ = None\\n\\n        if self.differentiation is not None:\\n            if not isinstance(differentiation, int) or differentiation < 1:\\n                raise ValueError(\\n                    f\"Argument `differentiation` must be an integer equal to or \"\\n                    f\"greater than 1. Got {differentiation}.\"\\n                )\\n            self.window_size += self.differentiation\\n            self.differentiator = TimeSeriesDifferentiator(\\n                order=self.differentiation, window_size=self.window_size\\n            )\\n\\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\\n            forecaster_name = type(self).__name__, \\n            regressor       = regressor, \\n            weight_func     = weight_func, \\n            series_weights  = None\\n        )\\n\\n        self.fit_kwargs = check_select_fit_kwargs(\\n                              regressor  = regressor,\\n                              fit_kwargs = fit_kwargs\\n                          )\\n\\n    def __repr__(\\n        self\\n    ) -> str:\\n        \"\"\"\\n        Information displayed when a ForecasterRecursive object is printed.\\n        \"\"\"\\n\\n        (\\n            params,\\n            _,\\n            _,\\n            exog_names_in_,\\n            _,\\n        ) = self._preprocess_repr(\\n                regressor      = self.regressor,\\n                exog_names_in_ = self.exog_names_in_\\n            )\\n        \\n        params = self._format_text_repr(params)\\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\\n\\n        info = (\\n            f\"{\\'=\\' * len(type(self).__name__)} \\\\n\"\\n            f\"{type(self).__name__} \\\\n\"\\n            f\"{\\'=\\' * len(type(self).__name__)} \\\\n\"\\n            f\"Regressor: {type(self.regressor).__name__} \\\\n\"\\n            f\"Lags: {self.lags} \\\\n\"\\n            f\"Window features: {self.window_features_names} \\\\n\"\\n            f\"Window size: {self.window_size} \\\\n\"\\n            f\"Exogenous included: {self.exog_in_} \\\\n\"\\n            f\"Exogenous names: {exog_names_in_} \\\\n\"\\n            f\"Transformer for y: {self.transformer_y} \\\\n\"\\n            f\"Transformer for exog: {self.transformer_exog} \\\\n\"\\n            f\"Weight function included: {True if self.weight_func is not None else False} \\\\n\"\\n            f\"Differentiation order: {self.differentiation} \\\\n\"\\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\\\n\"\\n            f\"Training index type: {str(self.index_type_).split(\\'.\\')[-1][:-2] if self.is_fitted else None} \\\\n\"\\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\\\n\"\\n            f\"Regressor parameters: {params} \\\\n\"\\n            f\"fit_kwargs: {self.fit_kwargs} \\\\n\"\\n            f\"Creation date: {self.creation_date} \\\\n\"\\n            f\"Last fit date: {self.fit_date} \\\\n\"\\n            f\"Skforecast version: {self.skforecast_version} \\\\n\"\\n            f\"Python version: {self.python_version} \\\\n\"\\n            f\"Forecaster id: {self.forecaster_id} \\\\n\"\\n        )\\n\\n        return info\\n\\n\\n    def _repr_html_(self):\\n        \"\"\"\\n        HTML representation of the object.\\n        The \"General Information\" section is expanded by default.\\n        \"\"\"\\n\\n        (\\n            params,\\n            _,\\n            _,\\n            exog_names_in_,\\n            _,\\n        ) = self._preprocess_repr(\\n                regressor      = self.regressor,\\n                exog_names_in_ = self.exog_names_in_\\n            )\\n\\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\\n        \\n        content = f\"\"\"\\n        <div class=\"container-{unique_id}\">\\n            <h2>{type(self).__name__}</h2>\\n            <details open>\\n                <summary>General Information</summary>\\n                <ul>\\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\\n                    <li><strong>Lags:</strong> {self.lags}</li>\\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\\n                    <li><strong>Window size:</strong> {self.window_size}</li>\\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\\n                    <li><strong>Python version:</strong> {self.python_version}</li>\\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Exogenous Variables</summary>\\n                <ul>\\n                    {exog_names_in_}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Data Transformations</summary>\\n                <ul>\\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Training Information</summary>\\n                <ul>\\n                    <li><strong>Training range:</strong> {self.training_range_.to_list() if self.is_fitted else \\'Not fitted\\'}</li>\\n                    <li><strong>Training index type:</strong> {str(self.index_type_).split(\\'.\\')[-1][:-2] if self.is_fitted else \\'Not fitted\\'}</li>\\n                    <li><strong>Training index frequency:</strong> {self.index_freq_ if self.is_fitted else \\'Not fitted\\'}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Regressor Parameters</summary>\\n                <ul>\\n                    {params}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Fit Kwargs</summary>\\n                <ul>\\n                    {self.fit_kwargs}\\n                </ul>\\n            </details>\\n            <p>\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\">&#128712 <strong>API Reference</strong></a>\\n                &nbsp;&nbsp;\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\">&#128462 <strong>User Guide</strong></a>\\n            </p>\\n        </div>\\n        \"\"\"\\n\\n        # Return the combined style and content\\n        return style + content\\n\\n\\n    def _create_lags(\\n        self,\\n        y: np.ndarray,\\n        X_as_pandas: bool = False,\\n        train_index: Optional[pd.Index] = None\\n    ) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\\n        \"\"\"\\n        Create the lagged values and their target variable from a time series.\\n        \\n        Note that the returned matrix `X_data` contains the lag 1 in the first \\n        column, the lag 2 in the in the second column and so on.\\n        \\n        Parameters\\n        ----------\\n        y : numpy ndarray\\n            Training time series values.\\n        X_as_pandas : bool, default `False`\\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\\n        train_index : pandas Index, default `None`\\n            Index of the training data. It is used to create the pandas DataFrame\\n            `X_data` when `X_as_pandas` is `True`.\\n\\n        Returns\\n        -------\\n        X_data : numpy ndarray, pandas DataFrame, None\\n            Lagged values (predictors).\\n        y_data : numpy ndarray\\n            Values of the time series related to each row of `X_data`.\\n        \\n        \"\"\"\\n\\n        X_data = None\\n        if self.lags is not None:\\n            n_rows = len(y) - self.window_size\\n            X_data = np.full(\\n                shape=(n_rows, len(self.lags)), fill_value=np.nan, order=\\'F\\', dtype=float\\n            )\\n            for i, lag in enumerate(self.lags):\\n                X_data[:, i] = y[self.window_size - lag: -lag]\\n\\n            if X_as_pandas:\\n                X_data = pd.DataFrame(\\n                             data    = X_data,\\n                             columns = self.lags_names,\\n                             index   = train_index\\n                         )\\n\\n        y_data = y[self.window_size:]\\n\\n        return X_data, y_data\\n\\n\\n    def _create_window_features(\\n        self, \\n        y: pd.Series,\\n        train_index: pd.Index,\\n        X_as_pandas: bool = False,\\n    ) -> Tuple[list, list]:\\n        \"\"\"\\n        \\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        train_index : pandas Index\\n            Index of the training data. It is used to create the pandas DataFrame\\n            `X_train_window_features` when `X_as_pandas` is `True`.\\n        X_as_pandas : bool, default `False`\\n            If `True`, the returned matrix `X_train_window_features` is a \\n            pandas DataFrame.\\n\\n        Returns\\n        -------\\n        X_train_window_features : list\\n            List of numpy ndarrays or pandas DataFrames with the window features.\\n        X_train_window_features_names_out_ : list\\n            Names of the window features.\\n        \\n        \"\"\"\\n\\n        len_train_index = len(train_index)\\n        X_train_window_features = []\\n        X_train_window_features_names_out_ = []\\n        for wf in self.window_features:\\n            X_train_wf = wf.transform_batch(y)\\n            if not isinstance(X_train_wf, pd.DataFrame):\\n                raise TypeError(\\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\\n                     f\"must return a pandas DataFrame.\")\\n                )\\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\\n            if not len(X_train_wf) == len_train_index:\\n                raise ValueError(\\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\\n                     f\"must return a DataFrame with the same number of rows as \"\\n                     f\"the input time series - `window_size`: {len_train_index}.\")\\n                )\\n            if not (X_train_wf.index == train_index).all():\\n                raise ValueError(\\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\\n                     f\"must return a DataFrame with the same index as \"\\n                     f\"the input time series - `window_size`.\")\\n                )\\n            \\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\\n            if not X_as_pandas:\\n                X_train_wf = X_train_wf.to_numpy()     \\n            X_train_window_features.append(X_train_wf)\\n\\n        return X_train_window_features, X_train_window_features_names_out_\\n\\n\\n    def _create_train_X_y(\\n        self,\\n        y: pd.Series,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\\n    ) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\\n        \"\"\"\\n        Create training matrices from univariate time series and exogenous\\n        variables.\\n        \\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `y` and their indexes must be aligned.\\n\\n        Returns\\n        -------\\n        X_train : pandas DataFrame\\n            Training values (predictors).\\n        y_train : pandas Series\\n            Values of the time series related to each row of `X_train`.\\n        exog_names_in_ : list\\n            Names of the exogenous variables used during training.\\n        X_train_window_features_names_out_ : list\\n            Names of the window features included in the matrix `X_train` created\\n            internally for training.\\n        X_train_exog_names_out_ : list\\n            Names of the exogenous variables included in the matrix `X_train` created\\n            internally for training. It can be different from `exog_names_in_` if\\n            some exogenous variables are transformed during the training process.\\n        X_train_features_names_out_ : list\\n            Names of the columns of the matrix created internally for training.\\n        exog_dtypes_in_ : dict\\n            Type of each exogenous variable/s used in training. If `transformer_exog` \\n            is used, the dtypes are calculated before the transformation.\\n        \\n        \"\"\"\\n\\n        check_y(y=y)\\n        y = input_to_frame(data=y, input_name=\\'y\\')\\n\\n        if len(y) <= self.window_size:\\n            raise ValueError(\\n                (f\"Length of `y` must be greater than the maximum window size \"\\n                 f\"needed by the forecaster.\\\\n\"\\n                 f\"    Length `y`: {len(y)}.\\\\n\"\\n                 f\"    Max window size: {self.window_size}.\\\\n\"\\n                 f\"    Lags window size: {self.max_lag}.\\\\n\"\\n                 f\"    Window features window size: {self.max_size_window_features}.\")\\n            )\\n\\n        fit_transformer = False if self.is_fitted else True\\n        y = transform_dataframe(\\n                df                = y, \\n                transformer       = self.transformer_y,\\n                fit               = fit_transformer,\\n                inverse_transform = False,\\n            )\\n        y_values, y_index = preprocess_y(y=y)\\n        train_index = y_index[self.window_size:]\\n\\n        if self.differentiation is not None:\\n            if not self.is_fitted:\\n                y_values = self.differentiator.fit_transform(y_values)\\n            else:\\n                differentiator = copy(self.differentiator)\\n                y_values = differentiator.fit_transform(y_values)\\n\\n        exog_names_in_ = None\\n        exog_dtypes_in_ = None\\n        categorical_features = False\\n        if exog is not None:\\n            check_exog(exog=exog, allow_nan=True)\\n            exog = input_to_frame(data=exog, input_name=\\'exog\\')\\n\\n            len_y = len(y_values)\\n            len_train_index = len(train_index)\\n            len_exog = len(exog)\\n            if not len_exog == len_y and not len_exog == len_train_index:\\n                raise ValueError(\\n                    f\"Length of `exog` must be equal to the length of `y` (if index is \"\\n                    f\"fully aligned) or length of `y` - `window_size` (if `exog` \"\\n                    f\"starts after the first `window_size` values).\\\\n\"\\n                    f\"    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\\\n\"\\n                    f\"    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\\\n\"\\n                    f\"    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})\"\\n                )\\n\\n            exog_names_in_ = exog.columns.to_list()\\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\\n\\n            exog = transform_dataframe(\\n                       df                = exog,\\n                       transformer       = self.transformer_exog,\\n                       fit               = fit_transformer,\\n                       inverse_transform = False\\n                   )\\n\\n            check_exog_dtypes(exog, call_check_exog=True)\\n            categorical_features = (\\n                exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\\n            )\\n\\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\\n            if len_exog == len_y:\\n                if not (exog_index == y_index).all():\\n                    raise ValueError(\\n                        \"When `exog` has the same length as `y`, the index of \"\\n                        \"`exog` must be aligned with the index of `y` \"\\n                        \"to ensure the correct alignment of values.\"\\n                    )\\n                # The first `self.window_size` positions have to be removed from \\n                # exog since they are not in X_train.\\n                exog = exog.iloc[self.window_size:, ]\\n            else:\\n                if not (exog_index == train_index).all():\\n                    raise ValueError(\\n                        \"When `exog` doesn\\'t contain the first `window_size` observations, \"\\n                        \"the index of `exog` must be aligned with the index of `y` minus \"\\n                        \"the first `window_size` observations to ensure the correct \"\\n                        \"alignment of values.\"\\n                    )\\n            \\n        X_train = []\\n        X_train_features_names_out_ = []\\n        X_as_pandas = True if categorical_features else False\\n\\n        X_train_lags, y_train = self._create_lags(\\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\\n        )\\n        if X_train_lags is not None:\\n            X_train.append(X_train_lags)\\n            X_train_features_names_out_.extend(self.lags_names)\\n        \\n        X_train_window_features_names_out_ = None\\n        if self.window_features is not None:\\n            n_diff = 0 if self.differentiation is None else self.differentiation\\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\\n            X_train_window_features, X_train_window_features_names_out_ = (\\n                self._create_window_features(\\n                    y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index\\n                )\\n            )\\n            X_train.extend(X_train_window_features)\\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\\n\\n        X_train_exog_names_out_ = None\\n        if exog is not None:\\n            X_train_exog_names_out_ = exog.columns.to_list()  \\n            if not X_as_pandas:\\n                exog = exog.to_numpy()     \\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\\n            X_train.append(exog)\\n        \\n        if len(X_train) == 1:\\n            X_train = X_train[0]\\n        else:\\n            if X_as_pandas:\\n                X_train = pd.concat(X_train, axis=1)\\n            else:\\n                X_train = np.concatenate(X_train, axis=1)\\n                \\n        if X_as_pandas:\\n            X_train.index = train_index\\n        else:\\n            X_train = pd.DataFrame(\\n                          data    = X_train,\\n                          index   = train_index,\\n                          columns = X_train_features_names_out_\\n                      )\\n        \\n        y_train = pd.Series(\\n                      data  = y_train,\\n                      index = train_index,\\n                      name  = \\'y\\'\\n                  )\\n\\n        return (\\n            X_train,\\n            y_train,\\n            exog_names_in_,\\n            X_train_window_features_names_out_,\\n            X_train_exog_names_out_,\\n            X_train_features_names_out_,\\n            exog_dtypes_in_\\n        )\\n\\n\\n    def create_train_X_y(\\n        self,\\n        y: pd.Series,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\\n    ) -> Tuple[pd.DataFrame, pd.Series]:\\n        \"\"\"\\n        Create training matrices from univariate time series and exogenous\\n        variables.\\n        \\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `y` and their indexes must be aligned.\\n\\n        Returns\\n        -------\\n        X_train : pandas DataFrame\\n            Training values (predictors).\\n        y_train : pandas Series\\n            Values of the time series related to each row of `X_data`.\\n        \\n        \"\"\"\\n\\n        output = self._create_train_X_y(y=y, exog=exog)\\n\\n        X_train = output[0]\\n        y_train = output[1]\\n\\n        return X_train, y_train\\n\\n\\n    def _train_test_split_one_step_ahead(\\n        self,\\n        y: pd.Series,\\n        initial_train_size: int,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\\n    ) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\\n        \"\"\"\\n        Create matrices needed to train and test the forecaster for one-step-ahead\\n        predictions.\\n\\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        initial_train_size : int\\n            Initial size of the training set. It is the number of observations used\\n            to train the forecaster before making the first prediction.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `y` and their indexes must be aligned.\\n        \\n        Returns\\n        -------\\n        X_train : pandas DataFrame\\n            Predictor values used to train the model.\\n        y_train : pandas Series\\n            Target values related to each row of `X_train`.\\n        X_test : pandas DataFrame\\n            Predictor values used to test the model.\\n        y_test : pandas Series\\n            Target values related to each row of `X_test`.\\n        \\n        \"\"\"\\n\\n        is_fitted = self.is_fitted\\n        self.is_fitted = False\\n        X_train, y_train, *_ = self._create_train_X_y(\\n            y    = y.iloc[: initial_train_size],\\n            exog = exog.iloc[: initial_train_size] if exog is not None else None\\n        )\\n\\n        test_init = initial_train_size - self.window_size\\n        self.is_fitted = True\\n        X_test, y_test, *_ = self._create_train_X_y(\\n            y    = y.iloc[test_init:],\\n            exog = exog.iloc[test_init:] if exog is not None else None\\n        )\\n\\n        self.is_fitted = is_fitted\\n\\n        return X_train, y_train, X_test, y_test\\n\\n\\n    def create_sample_weights(\\n        self,\\n        X_train: pd.DataFrame,\\n    ) -> np.ndarray:\\n        \"\"\"\\n        Crate weights for each observation according to the forecaster\\'s attribute\\n        `weight_func`.\\n\\n        Parameters\\n        ----------\\n        X_train : pandas DataFrame\\n            Dataframe created with the `create_train_X_y` method, first return.\\n\\n        Returns\\n        -------\\n        sample_weight : numpy ndarray\\n            Weights to use in `fit` method.\\n\\n        \"\"\"\\n\\n        sample_weight = None\\n\\n        if self.weight_func is not None:\\n            sample_weight = self.weight_func(X_train.index)\\n\\n        if sample_weight is not None:\\n            if np.isnan(sample_weight).any():\\n                raise ValueError(\\n                    \"The resulting `sample_weight` cannot have NaN values.\"\\n                )\\n            if np.any(sample_weight < 0):\\n                raise ValueError(\\n                    \"The resulting `sample_weight` cannot have negative values.\"\\n                )\\n            if np.sum(sample_weight) == 0:\\n                raise ValueError(\\n                    (\"The resulting `sample_weight` cannot be normalized because \"\\n                     \"the sum of the weights is zero.\")\\n                )\\n\\n        return sample_weight\\n\\n\\n    def fit(\\n        self,\\n        y: pd.Series,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        store_last_window: bool = True,\\n        store_in_sample_residuals: bool = True,\\n        random_state: int = 123\\n    ) -> None:\\n        \"\"\"\\n        Training Forecaster.\\n\\n        Additional arguments to be passed to the `fit` method of the regressor \\n        can be added with the `fit_kwargs` argument when initializing the forecaster.\\n        \\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `y` and their indexes must be aligned so\\n            that y[i] is regressed on exog[i].\\n        store_last_window : bool, default `True`\\n            Whether or not to store the last window (`last_window_`) of training data.\\n        store_in_sample_residuals : bool, default `True`\\n            If `True`, in-sample residuals will be stored in the forecaster object\\n            after fitting (`in_sample_residuals_` attribute).\\n        random_state : int, default `123`\\n            Set a seed for the random generator so that the stored sample \\n            residuals are always deterministic.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        \"\"\"\\n\\n        # Reset values in case the forecaster has already been fitted.\\n        self.last_window_                       = None\\n        self.index_type_                        = None\\n        self.index_freq_                        = None\\n        self.training_range_                    = None\\n        self.exog_in_                           = False\\n        self.exog_names_in_                     = None\\n        self.exog_type_in_                      = None\\n        self.exog_dtypes_in_                    = None\\n        self.X_train_window_features_names_out_ = None\\n        self.X_train_exog_names_out_            = None\\n        self.X_train_features_names_out_        = None\\n        self.in_sample_residuals_               = None\\n        self.is_fitted                          = False\\n        self.fit_date                           = None\\n\\n        (\\n            X_train,\\n            y_train,\\n            exog_names_in_,\\n            X_train_window_features_names_out_,\\n            X_train_exog_names_out_,\\n            X_train_features_names_out_,\\n            exog_dtypes_in_\\n        ) = self._create_train_X_y(y=y, exog=exog)\\n        sample_weight = self.create_sample_weights(X_train=X_train)\\n\\n        if sample_weight is not None:\\n            self.regressor.fit(\\n                X             = X_train,\\n                y             = y_train,\\n                sample_weight = sample_weight,\\n                **self.fit_kwargs\\n            )\\n        else:\\n            self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\\n\\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\\n        self.X_train_features_names_out_ = X_train_features_names_out_\\n\\n        self.is_fitted = True\\n        self.fit_date = pd.Timestamp.today().strftime(\\'%Y-%m-%d %H:%M:%S\\')\\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\\n        self.index_type_ = type(X_train.index)\\n        if isinstance(X_train.index, pd.DatetimeIndex):\\n            self.index_freq_ = X_train.index.freqstr\\n        else: \\n            self.index_freq_ = X_train.index.step\\n\\n        if exog is not None:\\n            self.exog_in_ = True\\n            self.exog_type_in_ = type(exog)\\n            self.exog_names_in_ = exog_names_in_\\n            self.exog_dtypes_in_ = exog_dtypes_in_\\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\\n\\n        # This is done to save time during fit in functions such as backtesting()\\n        if store_in_sample_residuals:\\n            self._binning_in_sample_residuals(\\n                y_true       = y_train.to_numpy(),\\n                y_pred       = self.regressor.predict(X_train).ravel(),\\n                random_state = random_state\\n            )\\n\\n        # The last time window of training data is stored so that lags needed as\\n        # predictors in the first iteration of `predict()` can be calculated. It\\n        # also includes the values need to calculate the diferenctiation.\\n        if store_last_window:\\n            self.last_window_ = (\\n                y.iloc[-self.window_size:]\\n                .copy()\\n                .to_frame(name=y.name if y.name is not None else \\'y\\')\\n            )\\n\\n\\n    def _binning_in_sample_residuals(\\n        self,\\n        y_true: np.ndarray,\\n        y_pred: np.ndarray,\\n        random_state: int = 123\\n    ) -> None:\\n        \"\"\"\\n        Binning residuals according to the predicted value each residual is\\n        associated with. First a skforecast.preprocessing.QuantileBinner object\\n        is fitted to the predicted values. Then, residuals are binned according\\n        to the predicted value each residual is associated with. Residuals are\\n        stored in the forecaster object as `in_sample_residuals_` and\\n        `in_sample_residuals_by_bin_`.\\n        If `transformer_y` is not `None`, `y_true` and `y_pred` are transformed\\n        before calculating residuals. If `differentiation` is not `None`, `y_true`\\n        and `y_pred` are differentiated before calculating residuals. If both,\\n        `transformer_y` and `differentiation` are not `None`, transformation is\\n        done before differentiation. The number of residuals stored per bin is\\n        limited to  `10_000 // self.binner.n_bins_`. The total number of residuals\\n        stored is `10_000`.\\n        **New in version 0.14.0**\\n\\n        Parameters\\n        ----------\\n        y_true : numpy ndarray\\n            True values of the time series.\\n        y_pred : numpy ndarray\\n            Predicted values of the time series.\\n        random_state : int, default `123`\\n            Set a seed for the random generator so that the stored sample \\n            residuals are always deterministic.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        \"\"\"\\n\\n        data = pd.DataFrame({\\'prediction\\': y_pred, \\'residuals\\': (y_true - y_pred)})\\n        data[\\'bin\\'] = self.binner.fit_transform(y_pred).astype(int)\\n        self.in_sample_residuals_by_bin_ = (\\n            data.groupby(\\'bin\\')[\\'residuals\\'].apply(np.array).to_dict()\\n        )\\n\\n        rng = np.random.default_rng(seed=random_state)\\n        max_sample = 10_000 // self.binner.n_bins_\\n        for k, v in self.in_sample_residuals_by_bin_.items():\\n            \\n            if len(v) > max_sample:\\n                sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\\n                self.in_sample_residuals_by_bin_[k] = sample\\n\\n        self.in_sample_residuals_ = np.concatenate(list(\\n            self.in_sample_residuals_by_bin_.values()\\n        ))\\n\\n        self.binner_intervals_ = self.binner.intervals_\\n\\n\\n    def _create_predict_inputs(\\n        self,\\n        steps: Union[int, str, pd.Timestamp], \\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        predict_boot: bool = False,\\n        use_in_sample_residuals: bool = True,\\n        use_binned_residuals: bool = False,\\n        check_inputs: bool = True,\\n    ) -> Tuple[np.ndarray, Optional[np.ndarray], pd.Index, int]:\\n        \"\"\"\\n        Create the inputs needed for the first iteration of the prediction \\n        process. As this is a recursive process, the last window is updated at \\n        each iteration of the prediction process.\\n        \\n        Parameters\\n        ----------\\n        steps : int, str, pandas Timestamp\\n            Number of steps to predict. \\n            \\n            + If steps is int, number of steps to predict. \\n            + If str or pandas Datetime, the prediction will be up to that date.\\n        last_window : pandas Series, pandas DataFrame, default `None`\\n            Series values used to create the predictors (lags) needed in the \\n            first iteration of the prediction (t + 1).\\n            If `last_window = None`, the values stored in `self.last_window_` are\\n            used to calculate the initial predictors, and the predictions start\\n            right after training data.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s.\\n        predict_boot : bool, default `False`\\n            If `True`, residuals are returned to generate bootstrapping predictions.\\n        use_in_sample_residuals : bool, default `True`\\n            If `True`, residuals from the training data are used as proxy of\\n            prediction error to create predictions. If `False`, out of sample \\n            residuals are used. In the latter case, the user should have\\n            calculated and stored the residuals within the forecaster (see\\n            `set_out_sample_residuals()`).\\n        use_binned_residuals : bool, default `False`\\n            If `True`, residuals used in each bootstrapping iteration are selected\\n            conditioning on the predicted values. If `False`, residuals are selected\\n            randomly without conditioning on the predicted values.\\n            **WARNING: This argument is newly introduced and requires special attention.\\n            It is still experimental and may undergo changes.\\n            **New in version 0.12.0**\\n        check_inputs : bool, default `True`\\n            If `True`, the input is checked for possible warnings and errors \\n            with the `check_predict_input` function. This argument is created \\n            for internal use and is not recommended to be changed.\\n\\n        Returns\\n        -------\\n        last_window_values : numpy ndarray\\n            Series values used to create the predictors needed in the first \\n            iteration of the prediction (t + 1).\\n        exog_values : numpy ndarray, None\\n            Exogenous variable/s included as predictor/s.\\n        prediction_index : pandas Index\\n            Index of the predictions.\\n        steps: int\\n            Number of future steps predicted.\\n        \\n        \"\"\"\\n\\n        if last_window is None:\\n            last_window = self.last_window_\\n\\n        if self.is_fitted:\\n            steps = date_to_index_position(\\n                        index        = last_window.index,\\n                        date_input   = steps,\\n                        date_literal = \\'steps\\'\\n                    )\\n\\n        if check_inputs:\\n            check_predict_input(\\n                forecaster_name  = type(self).__name__,\\n                steps            = steps,\\n                is_fitted        = self.is_fitted,\\n                exog_in_         = self.exog_in_,\\n                index_type_      = self.index_type_,\\n                index_freq_      = self.index_freq_,\\n                window_size      = self.window_size,\\n                last_window      = last_window,\\n                exog             = exog,\\n                exog_type_in_    = self.exog_type_in_,\\n                exog_names_in_   = self.exog_names_in_,\\n                interval         = None\\n            )\\n        \\n            if predict_boot and not use_in_sample_residuals:\\n                if not use_binned_residuals and self.out_sample_residuals_ is None:\\n                    raise ValueError(\\n                        \"`forecaster.out_sample_residuals_` is `None`. Use \"\\n                        \"`use_in_sample_residuals=True` or the \"\\n                        \"`set_out_sample_residuals()` method before predicting.\"\\n                    )\\n                if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\\n                    raise ValueError(\\n                        \"`forecaster.out_sample_residuals_by_bin_` is `None`. Use \"\\n                        \"`use_in_sample_residuals=True` or the \"\\n                        \"`set_out_sample_residuals()` method before predicting.\"\\n                    )\\n\\n        last_window = last_window.iloc[-self.window_size:].copy()\\n        last_window_values, last_window_index = preprocess_last_window(\\n                                                    last_window = last_window\\n                                                )\\n\\n        last_window_values = transform_numpy(\\n                                 array             = last_window_values,\\n                                 transformer       = self.transformer_y,\\n                                 fit               = False,\\n                                 inverse_transform = False\\n                             )\\n        if self.differentiation is not None:\\n            last_window_values = self.differentiator.fit_transform(last_window_values)\\n\\n        if exog is not None:\\n            exog = input_to_frame(data=exog, input_name=\\'exog\\')\\n            exog = exog.loc[:, self.exog_names_in_]\\n            exog = transform_dataframe(\\n                       df                = exog,\\n                       transformer       = self.transformer_exog,\\n                       fit               = False,\\n                       inverse_transform = False\\n                   )\\n            check_exog_dtypes(exog=exog)\\n            exog_values = exog.to_numpy()[:steps]\\n        else:\\n            exog_values = None\\n\\n        prediction_index = expand_index(\\n                               index = last_window_index,\\n                               steps = steps,\\n                           )\\n\\n        return last_window_values, exog_values, prediction_index, steps\\n\\n\\n    def _recursive_predict(\\n        self,\\n        steps: int,\\n        last_window_values: np.ndarray,\\n        exog_values: Optional[np.ndarray] = None,\\n        residuals: Optional[Union[np.ndarray, dict]] = None,\\n        use_binned_residuals: bool = False,\\n    ) -> np.ndarray:\\n        \"\"\"\\n        Predict n steps ahead. It is an iterative process in which, each prediction,\\n        is used as a predictor for the next step.\\n        \\n        Parameters\\n        ----------\\n        steps : int\\n            Number of future steps predicted.\\n        last_window_values : numpy ndarray\\n            Series values used to create the predictors needed in the first \\n            iteration of the prediction (t + 1).\\n        exog_values : numpy ndarray, default `None`\\n            Exogenous variable/s included as predictor/s.\\n        residuals : numpy ndarray, dict, default `None`\\n            Residuals used to generate bootstrapping predictions.\\n        use_binned_residuals : bool, default `False`\\n            If `True`, residuals used in each bootstrapping iteration are selected\\n            conditioning on the predicted values. If `False`, residuals are selected\\n            randomly without conditioning on the predicted values.\\n            **WARNING: This argument is newly introduced and requires special attention.\\n            It is still experimental and may undergo changes.\\n            **New in version 0.12.0**\\n\\n        Returns\\n        -------\\n        predictions : numpy ndarray\\n            Predicted values.\\n        \\n        \"\"\"\\n\\n        n_lags = len(self.lags) if self.lags is not None else 0\\n        n_window_features = (\\n            len(self.X_train_window_features_names_out_)\\n            if self.window_features is not None\\n            else 0\\n        )\\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\\n\\n        X = np.full(\\n            shape=(n_lags + n_window_features + n_exog), fill_value=np.nan, dtype=float\\n        )\\n        predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\\n        last_window = np.concatenate((last_window_values, predictions))\\n\\n        for i in range(steps):\\n\\n            if self.lags is not None:\\n                X[:n_lags] = last_window[-self.lags - (steps - i)]\\n            if self.window_features is not None:\\n                X[n_lags : n_lags + n_window_features] = np.concatenate(\\n                    [\\n                        wf.transform(last_window[i : -(steps - i)])\\n                        for wf in self.window_features\\n                    ]\\n                )\\n            if exog_values is not None:\\n                X[n_lags + n_window_features:] = exog_values[i]\\n        \\n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\\n            \\n            if residuals is not None:\\n                if use_binned_residuals:\\n                    predicted_bin = (\\n                        self.binner.transform(pred).item()\\n                    )\\n                    step_residual = residuals[predicted_bin][i]\\n                else:\\n                    step_residual = residuals[i]\\n                \\n                pred += step_residual\\n            \\n            predictions[i] = pred[0]\\n\\n            # Update `last_window` values. The first position is discarded and \\n            # the new prediction is added at the end.\\n            last_window[-(steps - i)] = pred[0]\\n\\n        return predictions\\n\\n\\n    def create_predict_X(\\n        self,\\n        steps: int,\\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\\n    ) -> pd.DataFrame:\\n        \"\"\"\\n        Create the predictors needed to predict `steps` ahead. As it is a recursive\\n        process, the predictors are created at each iteration of the prediction \\n        process.\\n        \\n        Parameters\\n        ----------\\n        steps : int, str, pandas Timestamp\\n            Number of steps to predict. \\n            \\n            + If steps is int, number of steps to predict. \\n            + If str or pandas Datetime, the prediction will be up to that date.\\n        last_window : pandas Series, pandas DataFrame, default `None`\\n            Series values used to create the predictors (lags) needed in the \\n            first iteration of the prediction (t + 1).\\n            If `last_window = None`, the values stored in `self.last_window_` are\\n            used to calculate the initial predictors, and the predictions start\\n            right after training data.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s.\\n\\n        Returns\\n        -------\\n        X_predict : pandas DataFrame\\n            Pandas DataFrame with the predictors for each step. The index \\n            is the same as the prediction index.\\n        \\n        \"\"\"\\n\\n        last_window_values, exog_values, prediction_index, steps = (\\n            self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\\n        )\\n        \\n        with warnings.catch_warnings():\\n            warnings.filterwarnings(\\n                \"ignore\", \\n                message=\"X does not have valid feature names\", \\n                category=UserWarning\\n            )\\n            predictions = self._recursive_predict(\\n                              steps              = steps,\\n                              last_window_values = last_window_values,\\n                              exog_values        = exog_values\\n                          )\\n\\n        X_predict = []\\n        full_predictors = np.concatenate((last_window_values, predictions))\\n\\n        if self.lags is not None:\\n            idx = np.arange(-steps, 0)[:, None] - self.lags\\n            X_lags = full_predictors[idx + len(full_predictors)]\\n            X_predict.append(X_lags)\\n\\n        if self.window_features is not None:\\n            X_window_features = np.full(\\n                shape      = (steps, len(self.X_train_window_features_names_out_)), \\n                fill_value = np.nan, \\n                order      = \\'C\\',\\n                dtype      = float\\n            )\\n            for i in range(steps):\\n                X_window_features[i, :] = np.concatenate(\\n                    [wf.transform(full_predictors[i:-(steps - i)]) \\n                     for wf in self.window_features]\\n                )\\n            X_predict.append(X_window_features)\\n\\n        if exog is not None:\\n            X_predict.append(exog_values)\\n\\n        X_predict = pd.DataFrame(\\n                        data    = np.concatenate(X_predict, axis=1),\\n                        columns = self.X_train_features_names_out_,\\n                        index   = prediction_index\\n                    )\\n        \\n        if self.transformer_y is not None or self.differentiation is not None:\\n            warnings.warn(\\n                \"The output matrix is in the transformed scale due to the \"\\n                \"inclusion of transformations or differentiation in the Forecaster. \"\\n                \"As a result, any predictions generated using this matrix will also \"\\n                \"be in the transformed scale. Please refer to the documentation \"\\n                \"for more details: \"\\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\\n                DataTransformationWarning\\n            )\\n\\n        return X_predict\\n\\n\\n    def predict(\\n        self,\\n        steps: Union[int, str, pd.Timestamp],\\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        check_inputs: bool = True\\n    ) -> pd.Series:\\n        \"\"\"\\n        Predict n steps ahead. It is an recursive process in which, each prediction,\\n        is used as a predictor for the next step.\\n        \\n        Parameters\\n        ----------\\n        steps : int, str, pandas Timestamp\\n            Number of steps to predict. \\n            \\n            + If steps is int, number of steps to predict. \\n            + If str or pandas Datetime, the prediction will be up to that date.\\n        last_window : pandas Series, pandas DataFrame, default `None`\\n            Series values used to create the predictors (lags) needed in the \\n            first iteration of the prediction (t + 1).\\n            If `last_window = None`, the values stored in `self.last_window_` are\\n            used to calculate the initial predictors, and the predictions start\\n            right after training data.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s.\\n        check_inputs : bool, default `True`\\n            If `True`, the input is checked for possible warnings and errors \\n            with the `check_predict_input` function. This argument is created \\n            for internal use and is not recommended to be changed.\\n\\n        Returns\\n        -------\\n        predictions : pandas Series\\n            Predicted values.\\n        \\n        \"\"\"\\n\\n        last_window_values, exog_values, prediction_index, steps = (\\n            self._create_predict_inputs(\\n                steps=steps,\\n                last_window=last_window,\\n                exog=exog,\\n                check_inputs=check_inputs,\\n            )\\n        )\\n\\n        with warnings.catch_warnings():\\n            warnings.filterwarnings(\\n                \"ignore\", \\n                message=\"X does not have valid feature names\", \\n                category=UserWarning\\n            )\\n            predictions = self._recursive_predict(\\n                              steps              = steps,\\n                              last_window_values = last_window_values,\\n                              exog_values        = exog_values\\n                          )\\n\\n        if self.differentiation is not None:\\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\\n\\n        predictions = transform_numpy(\\n                          array             = predictions,\\n                          transformer       = self.transformer_y,\\n                          fit               = False,\\n                          inverse_transform = True\\n                      )\\n\\n        predictions = pd.Series(\\n                          data  = predictions,\\n                          index = prediction_index,\\n                          name  = \\'pred\\'\\n                      )\\n\\n        return predictions\\n\\n\\n    def predict_bootstrapping(\\n        self,\\n        steps: Union[int, str, pd.Timestamp],\\n        last_window: Optional[pd.Series] = None,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        n_boot: int = 250,\\n        random_state: int = 123,\\n        use_in_sample_residuals: bool = True,\\n        use_binned_residuals: bool = False\\n    ) -> pd.DataFrame:\\n        \"\"\"\\n        Generate multiple forecasting predictions using a bootstrapping process. \\n        By sampling from a collection of past observed errors (the residuals),\\n        each iteration of bootstrapping generates a different set of predictions. \\n        See the Notes section for more information. \\n        \\n        Parameters\\n        ----------\\n        steps : int, str, pandas Timestamp\\n            Number of steps to predict. \\n            \\n            + If steps is int, number of steps to predict. \\n            + If str or pandas Datetime, the prediction will be up to that date.\\n        last_window : pandas Series, default `None`\\n            Series values used to create the predictors (lags) needed in the \\n            first iteration of the prediction (t + 1).\\n            If `last_window = None`, the values stored in `self.last_window_` are\\n            used to calculate the initial predictors, and the predictions start\\n            right after training data.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s.\\n        n_boot : int, default `250`\\n            Number of bootstrapping iterations used to estimate predictions.\\n        random_state : int, default `123`\\n            Sets a seed to the random generator, so that boot predictions are always \\n            deterministic.\\n        use_in_sample_residuals : bool, default `True`\\n            If `True`, residuals from the training data are used as proxy of\\n            prediction error to create predictions. If `False`, out of sample \\n            residuals are used. In the latter case, the user should have\\n            calculated and stored the residuals within the forecaster (see\\n            `set_out_sample_residuals()`).\\n        use_binned_residuals : bool, default `False`\\n            If `True`, residuals used in each bootstrapping iteration are selected\\n            conditioning on the predicted values. If `False`, residuals are selected\\n            randomly without conditioning on the predicted values.\\n            **WARNING: This argument is newly introduced and requires special attention.\\n            It is still experimental and may undergo changes.\\n            **New in version 0.12.0**\\n\\n        Returns\\n        -------\\n        boot_predictions : pandas DataFrame\\n            Predictions generated by bootstrapping.\\n            Shape: (steps, n_boot)\\n\\n        Notes\\n        -----\\n        More information about prediction intervals in forecasting:\\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\\n\\n        \"\"\"\\n\\n        (\\n            last_window_values,\\n            exog_values,\\n            prediction_index,\\n            steps\\n        ) = self._create_predict_inputs(\\n            steps                   = steps, \\n            last_window             = last_window, \\n            exog                    = exog,\\n            predict_boot            = True, \\n            use_in_sample_residuals = use_in_sample_residuals,\\n            use_binned_residuals    = use_binned_residuals\\n        )\\n\\n        if use_in_sample_residuals:\\n            residuals = self.in_sample_residuals_\\n            residuals_by_bin = self.in_sample_residuals_by_bin_\\n        else:\\n            residuals = self.out_sample_residuals_\\n            residuals_by_bin = self.out_sample_residuals_by_bin_\\n\\n        rng = np.random.default_rng(seed=random_state)\\n        if use_binned_residuals:\\n            sampled_residuals = {\\n                k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))]\\n                for k, v in residuals_by_bin.items()\\n            }\\n        else:\\n            sampled_residuals = residuals[\\n                rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\\n            ]\\n        \\n        boot_columns = []\\n        boot_predictions = np.full(\\n                               shape      = (steps, n_boot),\\n                               fill_value = np.nan,\\n                               order      = \\'F\\',\\n                               dtype      = float\\n                           )\\n        with warnings.catch_warnings():\\n            warnings.filterwarnings(\\n                \"ignore\", \\n                message=\"X does not have valid feature names\", \\n                category=UserWarning\\n            )\\n            for i in range(n_boot):\\n\\n                if use_binned_residuals:\\n                    boot_sampled_residuals = {\\n                        k: v[:, i]\\n                        for k, v in sampled_residuals.items()\\n                    }\\n                else:\\n                    boot_sampled_residuals = sampled_residuals[:, i]\\n\\n                boot_columns.append(f\"pred_boot_{i}\")\\n                boot_predictions[:, i] = self._recursive_predict(\\n                    steps                = steps,\\n                    last_window_values   = last_window_values,\\n                    exog_values          = exog_values,\\n                    residuals            = boot_sampled_residuals,\\n                    use_binned_residuals = use_binned_residuals,\\n                )\\n\\n        if self.differentiation is not None:\\n            boot_predictions = (\\n                self.differentiator.inverse_transform_next_window(boot_predictions)\\n            )\\n        \\n        if self.transformer_y:\\n            boot_predictions = np.apply_along_axis(\\n                                   func1d            = transform_numpy,\\n                                   axis              = 0,\\n                                   arr               = boot_predictions,\\n                                   transformer       = self.transformer_y,\\n                                   fit               = False,\\n                                   inverse_transform = True\\n                               )\\n\\n        boot_predictions = pd.DataFrame(\\n                               data    = boot_predictions,\\n                               index   = prediction_index,\\n                               columns = boot_columns\\n                           )\\n\\n        return boot_predictions\\n\\n\\n    def predict_interval(\\n        self,\\n        steps: Union[int, str, pd.Timestamp],\\n        last_window: Optional[pd.Series] = None,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        interval: list = [5, 95],\\n        n_boot: int = 250,\\n        random_state: int = 123,\\n        use_in_sample_residuals: bool = True,\\n        use_binned_residuals: bool = False\\n    ) -> pd.DataFrame:\\n        \"\"\"\\n        Iterative process in which each prediction is used as a predictor\\n        for the next step, and bootstrapping is used to estimate prediction\\n        intervals. Both predictions and intervals are returned.\\n        \\n        Parameters\\n        ----------\\n        steps : int, str, pandas Timestamp\\n            Number of steps to predict. \\n            \\n            + If steps is int, number of steps to predict. \\n            + If str or pandas Datetime, the prediction will be up to that date.\\n        last_window : pandas Series, default `None`\\n            Series values used to create the predictors (lags) needed in the \\n            first iteration of the prediction (t + 1).\\n            If `last_window = None`, the values stored in` self.last_window_` are\\n            used to calculate the initial predictors, and the predictions start\\n            right after training data.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s.\\n        interval : list, default `[5, 95]`\\n            Confidence of the prediction interval estimated. Sequence of \\n            percentiles to compute, which must be between 0 and 100 inclusive. \\n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\\n        n_boot : int, default `250`\\n            Number of bootstrapping iterations used to estimate predictions.\\n        random_state : int, default `123`\\n            Sets a seed to the random generator, so that boot predictions are always \\n            deterministic.\\n        use_in_sample_residuals : bool, default `True`\\n            If `True`, residuals from the training data are used as proxy of\\n            prediction error to create predictions. If `False`, out of sample \\n            residuals are used. In the latter case, the user should have\\n            calculated and stored the residuals within the forecaster (see\\n            `set_out_sample_residuals()`).\\n        use_binned_residuals : bool, default `False`\\n            If `True`, residuals used in each bootstrapping iteration are selected\\n            conditioning on the predicted values. If `False`, residuals are selected\\n            randomly without conditioning on the predicted values.\\n            **WARNING: This argument is newly introduced and requires special attention.\\n            It is still experimental and may undergo changes.\\n            **New in version 0.12.0**\\n\\n        Returns\\n        -------\\n        predictions : pandas DataFrame\\n            Values predicted by the forecaster and their estimated interval.\\n\\n            - pred: predictions.\\n            - lower_bound: lower bound of the interval.\\n            - upper_bound: upper bound of the interval.\\n\\n        Notes\\n        -----\\n        More information about prediction intervals in forecasting:\\n        https://otexts.com/fpp2/prediction-intervals.html\\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\\n        George Athanasopoulos.\\n        \\n        \"\"\"\\n\\n        check_interval(interval=interval)\\n\\n        boot_predictions = self.predict_bootstrapping(\\n                               steps                   = steps,\\n                               last_window             = last_window,\\n                               exog                    = exog,\\n                               n_boot                  = n_boot,\\n                               random_state            = random_state,\\n                               use_in_sample_residuals = use_in_sample_residuals,\\n                               use_binned_residuals    = use_binned_residuals\\n                           )\\n\\n        predictions = self.predict(\\n                          steps        = steps,\\n                          last_window  = last_window,\\n                          exog         = exog,\\n                          check_inputs = False\\n                      )\\n\\n        interval = np.array(interval) / 100\\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\\n        predictions_interval.columns = [\\'lower_bound\\', \\'upper_bound\\']\\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\\n\\n        return predictions\\n\\n\\n    def predict_quantiles(\\n        self,\\n        steps: Union[int, str, pd.Timestamp],\\n        last_window: Optional[pd.Series] = None,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        quantiles: list = [0.05, 0.5, 0.95],\\n        n_boot: int = 250,\\n        random_state: int = 123,\\n        use_in_sample_residuals: bool = True,\\n        use_binned_residuals: bool = False\\n    ) -> pd.DataFrame:\\n        \"\"\"\\n        Calculate the specified quantiles for each step. After generating \\n        multiple forecasting predictions through a bootstrapping process, each \\n        quantile is calculated for each step.\\n        \\n        Parameters\\n        ----------\\n        steps : int, str, pandas Timestamp\\n            Number of steps to predict. \\n            \\n            + If steps is int, number of steps to predict. \\n            + If str or pandas Datetime, the prediction will be up to that date.\\n        last_window : pandas Series, default `None`\\n            Series values used to create the predictors (lags) needed in the \\n            first iteration of the prediction (t + 1).\\n            If `last_window = None`, the values stored in` self.last_window_` are\\n            used to calculate the initial predictors, and the predictions start\\n            right after training data.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s.\\n        quantiles : list, default `[0.05, 0.5, 0.95]`\\n            Sequence of quantiles to compute, which must be between 0 and 1 \\n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \\n            `quantiles = [0.05, 0.5, 0.95]`.\\n        n_boot : int, default `250`\\n            Number of bootstrapping iterations used to estimate quantiles.\\n        random_state : int, default `123`\\n            Sets a seed to the random generator, so that boot quantiles are always \\n            deterministic.\\n        use_in_sample_residuals : bool, default `True`\\n            If `True`, residuals from the training data are used as proxy of\\n            prediction error to create prediction quantiles. If `False`, out of\\n            sample residuals are used. In the latter case, the user should have\\n            calculated and stored the residuals within the forecaster (see\\n            `set_out_sample_residuals()`).\\n        use_binned_residuals : bool, default `False`\\n            If `True`, residuals used in each bootstrapping iteration are selected\\n            conditioning on the predicted values. If `False`, residuals are selected\\n            randomly without conditioning on the predicted values.\\n            **WARNING: This argument is newly introduced and requires special attention.\\n            It is still experimental and may undergo changes.\\n            **New in version 0.12.0**\\n\\n        Returns\\n        -------\\n        predictions : pandas DataFrame\\n            Quantiles predicted by the forecaster.\\n\\n        Notes\\n        -----\\n        More information about prediction intervals in forecasting:\\n        https://otexts.com/fpp2/prediction-intervals.html\\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\\n        George Athanasopoulos.\\n        \\n        \"\"\"\\n\\n        check_interval(quantiles=quantiles)\\n\\n        boot_predictions = self.predict_bootstrapping(\\n                               steps                   = steps,\\n                               last_window             = last_window,\\n                               exog                    = exog,\\n                               n_boot                  = n_boot,\\n                               random_state            = random_state,\\n                               use_in_sample_residuals = use_in_sample_residuals,\\n                               use_binned_residuals    = use_binned_residuals\\n                           )\\n\\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\\n        predictions.columns = [f\\'q_{q}\\' for q in quantiles]\\n\\n        return predictions\\n\\n\\n    def predict_dist(\\n        self,\\n        steps: Union[int, str, pd.Timestamp],\\n        distribution: object,\\n        last_window: Optional[pd.Series] = None,\\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\\n        n_boot: int = 250,\\n        random_state: int = 123,\\n        use_in_sample_residuals: bool = True,\\n        use_binned_residuals: bool = False\\n    ) -> pd.DataFrame:\\n        \"\"\"\\n        Fit a given probability distribution for each step. After generating \\n        multiple forecasting predictions through a bootstrapping process, each \\n        step is fitted to the given distribution.\\n        \\n        Parameters\\n        ----------\\n        steps : int, str, pandas Timestamp\\n            Number of steps to predict. \\n            \\n            + If steps is int, number of steps to predict. \\n            + If str or pandas Datetime, the prediction will be up to that date.\\n        distribution : Object\\n            A distribution object from scipy.stats.\\n        last_window : pandas Series, default `None`\\n            Series values used to create the predictors (lags) needed in the \\n            first iteration of the prediction (t + 1).  \\n            If `last_window = None`, the values stored in` self.last_window_` are\\n            used to calculate the initial predictors, and the predictions start\\n            right after training data.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s.\\n        n_boot : int, default `250`\\n            Number of bootstrapping iterations used to estimate predictions.\\n        random_state : int, default `123`\\n            Sets a seed to the random generator, so that boot predictions are always \\n            deterministic.\\n        use_in_sample_residuals : bool, default `True`\\n            If `True`, residuals from the training data are used as proxy of\\n            prediction error to create predictions. If `False`, out of sample \\n            residuals are used. In the latter case, the user should have\\n            calculated and stored the residuals within the forecaster (see\\n            `set_out_sample_residuals()`).\\n        use_binned_residuals : bool, default `False`\\n            If `True`, residuals used in each bootstrapping iteration are selected\\n            conditioning on the predicted values. If `False`, residuals are selected\\n            randomly without conditioning on the predicted values.\\n            **WARNING: This argument is newly introduced and requires special attention.\\n            It is still experimental and may undergo changes.\\n            **New in version 0.12.0**\\n\\n        Returns\\n        -------\\n        predictions : pandas DataFrame\\n            Distribution parameters estimated for each step.\\n\\n        \"\"\"\\n\\n        boot_samples = self.predict_bootstrapping(\\n                           steps                   = steps,\\n                           last_window             = last_window,\\n                           exog                    = exog,\\n                           n_boot                  = n_boot,\\n                           random_state            = random_state,\\n                           use_in_sample_residuals = use_in_sample_residuals,\\n                           use_binned_residuals    = use_binned_residuals\\n                       )       \\n\\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters\\n                       if not p == \\'x\\'] + [\"loc\", \"scale\"]\\n        param_values = np.apply_along_axis(\\n                           lambda x: distribution.fit(x),\\n                           axis = 1,\\n                           arr  = boot_samples\\n                       )\\n        predictions = pd.DataFrame(\\n                          data    = param_values,\\n                          columns = param_names,\\n                          index   = boot_samples.index\\n                      )\\n\\n        return predictions\\n\\n    def set_params(\\n        self, \\n        params: dict\\n    ) -> None:\\n        \"\"\"\\n        Set new values to the parameters of the scikit learn model stored in the\\n        forecaster.\\n        \\n        Parameters\\n        ----------\\n        params : dict\\n            Parameters values.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        \"\"\"\\n\\n        self.regressor = clone(self.regressor)\\n        self.regressor.set_params(**params)\\n\\n    def set_fit_kwargs(\\n        self, \\n        fit_kwargs: dict\\n    ) -> None:\\n        \"\"\"\\n        Set new values for the additional keyword arguments passed to the `fit` \\n        method of the regressor.\\n        \\n        Parameters\\n        ----------\\n        fit_kwargs : dict\\n            Dict of the form {\"argument\": new_value}.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        \"\"\"\\n\\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\\n\\n    def set_lags(\\n        self, \\n        lags: Optional[Union[int, list, np.ndarray, range]] = None\\n    ) -> None:\\n        \"\"\"\\n        Set new value to the attribute `lags`. Attributes `lags_names`, \\n        `max_lag` and `window_size` are also updated.\\n        \\n        Parameters\\n        ----------\\n        lags : int, list, numpy ndarray, range, default `None`\\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \\n        \\n            - `int`: include lags from 1 to `lags` (included).\\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \\n            `lags`, all elements must be int.\\n            - `None`: no lags are included as predictors. \\n\\n        Returns\\n        -------\\n        None\\n        \\n        \"\"\"\\n\\n        if self.window_features is None and lags is None:\\n            raise ValueError(\\n                \"At least one of the arguments `lags` or `window_features` \"\\n                \"must be different from None. This is required to create the \"\\n                \"predictors used in training the forecaster.\"\\n            )\\n        \\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\\n        self.window_size = max(\\n            [ws for ws in [self.max_lag, self.max_size_window_features] \\n             if ws is not None]\\n        )\\n        if self.differentiation is not None:\\n            self.window_size += self.differentiation\\n            self.differentiator.set_params(window_size=self.window_size)\\n\\n    def set_window_features(\\n        self, \\n        window_features: Optional[Union[object, list]] = None\\n    ) -> None:\\n        \"\"\"\\n        Set new value to the attribute `window_features`. Attributes \\n        `max_size_window_features`, `window_features_names`, \\n        `window_features_class_names` and `window_size` are also updated.\\n        \\n        Parameters\\n        ----------\\n        window_features : object, list, default `None`\\n            Instance or list of instances used to create window features. Window features\\n            are created from the original time series and are included as predictors.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        \"\"\"\\n\\n        if window_features is None and self.lags is None:\\n            raise ValueError(\\n                \"At least one of the arguments `lags` or `window_features` \"\\n                \"must be different from None. This is required to create the \"\\n                \"predictors used in training the forecaster.\"\\n            )\\n        \\n        self.window_features, self.window_features_names, self.max_size_window_features = (\\n            initialize_window_features(window_features)\\n        )\\n        self.window_features_class_names = None\\n        if window_features is not None:\\n            self.window_features_class_names = [\\n                type(wf).__name__ for wf in self.window_features\\n            ] \\n        self.window_size = max(\\n            [ws for ws in [self.max_lag, self.max_size_window_features] \\n             if ws is not None]\\n        )\\n        if self.differentiation is not None:\\n            self.window_size += self.differentiation\\n            self.differentiator.set_params(window_size=self.window_size)\\n\\n    def set_out_sample_residuals(\\n        self,\\n        y_true: Union[pd.Series, np.ndarray],\\n        y_pred: Union[pd.Series, np.ndarray],\\n        append: bool = False,\\n        random_state: int = 123\\n    ) -> None:\\n        \"\"\"\\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\\n        residuals are meant to be calculated using observations that did not\\n        participate in the training process. `y_true` and `y_pred` are expected\\n        to be in the original scale of the time series. Residuals are calculated\\n        as `y_true` - `y_pred`, after applying the necessary transformations and\\n        differentiations if the forecaster includes them (`self.transformer_y`\\n        and `self.differentiation`). Two internal attributes are updated:\\n\\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\\n        predicted value they are associated with and stored in a dictionary, where\\n        the keys are the  intervals of the predicted values and the values are\\n        the residuals associated with that range. If a bin binning is empty, it\\n        is filled with a random sample of residuals from other bins. This is done\\n        to ensure that all bins have at least one residual and can be used in the\\n        prediction process.\\n\\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\\n        If the number of residuals is greater than 10_000, a random sample of\\n        10_000 residuals is stored. The number of residuals stored per bin is\\n        limited to `10_000 // self.binner.n_bins_`.\\n        \\n        Parameters\\n        ----------\\n        y_true : pandas Series, numpy ndarray, default `None`\\n            True values of the time series from which the residuals have been\\n            calculated.\\n        y_pred : pandas Series, numpy ndarray, default `None`\\n            Predicted values of the time series.\\n        append : bool, default `False`\\n            If `True`, new residuals are added to the once already stored in the\\n            forecaster. If after appending the new residuals, the limit of\\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\\n            sample of residuals is stored.\\n        random_state : int, default `123`\\n            Sets a seed to the random sampling for reproducible output.\\n\\n        Returns\\n        -------\\n        None\\n\\n        \"\"\"\\n\\n        if not self.is_fitted:\\n            raise NotFittedError(\\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\\n                \"arguments before using `set_out_sample_residuals()`.\"\\n            )\\n\\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\\n            raise TypeError(\\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\\n                f\"Got {type(y_true)}.\"\\n            )\\n        \\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\\n            raise TypeError(\\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\\n                f\"Got {type(y_pred)}.\"\\n            )\\n        \\n        if len(y_true) != len(y_pred):\\n            raise ValueError(\\n                f\"`y_true` and `y_pred` must have the same length. \"\\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\\n            )\\n        \\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\\n            if not y_true.index.equals(y_pred.index):\\n                raise ValueError(\\n                    \"`y_true` and `y_pred` must have the same index.\"\\n                )\\n\\n        if not isinstance(y_pred, np.ndarray):\\n            y_pred = y_pred.to_numpy()\\n\\n        if not isinstance(y_true, np.ndarray):\\n            y_true = y_true.to_numpy()\\n\\n        if self.transformer_y:\\n            y_true = transform_numpy(\\n                         array             = y_true,\\n                         transformer       = self.transformer_y,\\n                         fit               = False,\\n                         inverse_transform = False\\n                     )\\n            y_pred = transform_numpy(\\n                         array             = y_pred,\\n                         transformer       = self.transformer_y,\\n                         fit               = False,\\n                         inverse_transform = False\\n                     )\\n        \\n        if self.differentiation is not None:\\n            differentiator = copy(self.differentiator)\\n            differentiator.set_params(window_size=None)\\n            y_true = differentiator.fit_transform(y_true)[self.differentiation:]\\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\\n        \\n        residuals = y_true - y_pred\\n        data = pd.DataFrame({\\'prediction\\': y_pred, \\'residuals\\': residuals})\\n        data[\\'bin\\'] = self.binner.transform(y_pred).astype(int)\\n        residuals_by_bin = data.groupby(\\'bin\\')[\\'residuals\\'].apply(np.array).to_dict()\\n\\n        if append and self.out_sample_residuals_by_bin_ is not None:\\n            for k, v in residuals_by_bin.items():\\n                if k in self.out_sample_residuals_by_bin_:\\n                    self.out_sample_residuals_by_bin_[k] = np.concatenate((\\n                        self.out_sample_residuals_by_bin_[k], v)\\n                    )\\n                else:\\n                    self.out_sample_residuals_by_bin_[k] = v\\n        else:\\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\\n\\n        max_samples = 10_000 // self.binner.n_bins_\\n        rng = np.random.default_rng(seed=random_state)\\n        for k, v in self.out_sample_residuals_by_bin_.items():\\n            if len(v) > max_samples:\\n                sample = rng.choice(a=v, size=max_samples, replace=False)\\n                self.out_sample_residuals_by_bin_[k] = sample\\n\\n        for k in self.in_sample_residuals_by_bin_.keys():\\n            if k not in self.out_sample_residuals_by_bin_:\\n                self.out_sample_residuals_by_bin_[k] = np.array([])\\n\\n        empty_bins = [\\n            k for k, v in self.out_sample_residuals_by_bin_.items() \\n            if len(v) == 0\\n        ]\\n        if empty_bins:\\n            warnings.warn(\\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\\n                f\"No predicted values fall in the interval \"\\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\\n                f\"Empty bins will be filled with a random sample of residuals.\"\\n            )\\n            for k in empty_bins:\\n                self.out_sample_residuals_by_bin_[k] = rng.choice(\\n                    a       = residuals,\\n                    size    = max_samples,\\n                    replace = True\\n                )\\n\\n        self.out_sample_residuals_ = np.concatenate(list(\\n                                         self.out_sample_residuals_by_bin_.values()\\n                                     ))\\n\\n    def get_feature_importances(\\n        self,\\n        sort_importance: bool = True\\n    ) -> pd.DataFrame:\\n        \"\"\"\\n        Return feature importances of the regressor stored in the forecaster.\\n        Only valid when regressor stores internally the feature importances in the\\n        attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\\n\\n        Parameters\\n        ----------\\n        sort_importance: bool, default `True`\\n            If `True`, sorts the feature importances in descending order.\\n\\n        Returns\\n        -------\\n        feature_importances : pandas DataFrame\\n            Feature importances associated with each predictor.\\n\\n        \"\"\"\\n\\n        if not self.is_fitted:\\n            raise NotFittedError(\\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\\n                \"arguments before using `get_feature_importances()`.\"\\n            )\\n\\n        if isinstance(self.regressor, Pipeline):\\n            estimator = self.regressor[-1]\\n        else:\\n            estimator = self.regressor\\n\\n        if hasattr(estimator, \\'feature_importances_\\'):\\n            feature_importances = estimator.feature_importances_\\n        elif hasattr(estimator, \\'coef_\\'):\\n            feature_importances = estimator.coef_\\n        else:\\n            warnings.warn(\\n                f\"Impossible to access feature importances for regressor of type \"\\n                f\"{type(estimator)}. This method is only valid when the \"\\n                f\"regressor stores internally the feature importances in the \"\\n                f\"attribute `feature_importances_` or `coef_`.\"\\n            )\\n            feature_importances = None\\n\\n        if feature_importances is not None:\\n            feature_importances = pd.DataFrame({\\n                                      \\'feature\\': self.X_train_features_names_out_,\\n                                      \\'importance\\': feature_importances\\n                                  })\\n            if sort_importance:\\n                feature_importances = feature_importances.sort_values(\\n                                          by=\\'importance\\', ascending=False\\n                                      )\\n\\n        return feature_importances'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To modify the `ForecasterRecursive` class to receive a string argument for selecting different 'differentiator's, you can follow these steps:\n",
       "\n",
       "1. Add a new parameter `differentiator_type` to the `__init__` method to specify the type of differentiator.\n",
       "2. Modify the initialization of the `differentiator` attribute based on the `differentiator_type` argument.\n",
       "3. Update the relevant methods to use the selected differentiator.\n",
       "\n",
       "Here is a code snippet illustrating these changes:\n",
       "\n",
       "```python\n",
       "class ForecasterRecursive(ForecasterBase):\n",
       "    def __init__(\n",
       "        self,\n",
       "        regressor: object,\n",
       "        lags: Optional[Union[int, list, np.ndarray, range]] = None,\n",
       "        window_features: Optional[Union[object, list]] = None,\n",
       "        transformer_y: Optional[object] = None,\n",
       "        transformer_exog: Optional[object] = None,\n",
       "        weight_func: Optional[Callable] = None,\n",
       "        differentiation: Optional[int] = None,\n",
       "        differentiator_type: str = 'default',  # New parameter\n",
       "        fit_kwargs: Optional[dict] = None,\n",
       "        binner_kwargs: Optional[dict] = None,\n",
       "        forecaster_id: Optional[Union[str, int]] = None\n",
       "    ) -> None:\n",
       "        # Existing initialization code...\n",
       "\n",
       "        # Initialize differentiator based on differentiator_type\n",
       "        if self.differentiation is not None:\n",
       "            if not isinstance(differentiation, int) or differentiation < 1:\n",
       "                raise ValueError(\n",
       "                    f\"Argument `differentiation` must be an integer equal to or \"\n",
       "                    f\"greater than 1. Got {differentiation}.\"\n",
       "                )\n",
       "            self.window_size += self.differentiation\n",
       "\n",
       "            if differentiator_type == 'default':\n",
       "                self.differentiator = TimeSeriesDifferentiator(\n",
       "                    order=self.differentiation, window_size=self.window_size\n",
       "                )\n",
       "            elif differentiator_type == 'alternative':\n",
       "                self.differentiator = AlternativeDifferentiator(\n",
       "                    order=self.differentiation, window_size=self.window_size\n",
       "                )\n",
       "            else:\n",
       "                raise ValueError(f\"Unknown differentiator_type: {differentiator_type}\")\n",
       "\n",
       "    # Update other methods if necessary to use self.differentiator\n",
       "```\n",
       "\n",
       "In this example, the `differentiator_type` parameter allows you to choose between different differentiator implementations, such as `TimeSeriesDifferentiator` and `AlternativeDifferentiator`. You can add more differentiator types as needed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def print_markdown(text):\n",
    "    display(Markdown(text))\n",
    "\n",
    "def prettyCypherChain(question: str):\n",
    "    response = cypherChain.run(question)\n",
    "    print_markdown(response)\n",
    "\n",
    "prettyCypherChain(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
